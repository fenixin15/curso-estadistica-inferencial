---
title: "Módulo 7 Estadística - Tema 3"
author: "Ricardo Alberich Martí, Juan Gabriel Gomila Salas, Arnau Mir Torres."
  
output:
  pdf_document:
    toc: yes
  word_document:
    fig_caption: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
---

# Estimación puntual


## Conceptos básicos de muestreo 

```{r, include=FALSE}
options(digits = 17)
```

En todo estudio estadístico distinguiremos entre **población**, (conjunto de sujetos con una o varias características que podemos medir y deseamos estudiar), y **muestra**, (subconjunto de una población.)
Por ejemplo, si quisiéramos estudiar el peso de los estudiantes de primer curso de un cierto grado de una cierta universidad, entenderíamos que éstos forman la población de interés, y si escogemos al azar 10 estudiantes de primer grado grado, obtendremos una muestra de esta población. 

Existen dos tipos de análisis estadístico:

*  **Exploratorio** o  **descriptivo**: su objetivo es resumir, representar y explicar los datos de la muestra. Para llevarlo a cabo, se usan técnicas de  **estadística  descriptiva**. 

* **Inferencial** o **confirmatorio**: su objetivo es deducir (**inferir**),  a partir de los datos de la muestra, información significativa sobre el total de la población. A menudo esta inferencia pasa por contrastar una hipótesis sobre alguna propiedad de las características de la población. Las técnicas que se usan en este caso forman la **estadística  inferencial**.


Un estudio inferencial suele desglosarse en los pasos siguientes:
  
1. Establecer la característica que se desea estimar o la hipótesis que se desea contrastar.

2. Determinar la información (los datos) que se necesita para hacerlo.

3. Diseñar un experimento que permita recoger estos datos; este paso incluye:

    * Decidir qué tipo de muestra se va a tomar y su tamaño.
   
    * Elegir las técnicas adecuadas para realizar las inferencias deseadas a partir de la muestra que se tomará.
   
4. Tomar una muestra y medir los datos deseados sobre los individuos que la forman.

5. Aplicar las técnicas de inferencia elegidas con el *software* adecuado.

6. Obtener conclusiones.

7.  Si las conclusiones son fiables y suficientes, redactar un informe; en caso contrario, volver a empezar.


### Tipos de muestreo 

Existen muchos tipos de muestreo, cada uno de los cuales proporciona una muestra representativa de la población en algún sentido. A continuación describimos de forma breve algunas de estas técnicas.

#### Muestreo aleatorio con y sin reposición {-}

Un **muestreo aleatorio** consiste en seleccionar una muestra de la población de manera que todas las muestras del mismo tamaño sean **equiprobables**; es decir, que si fijamos el número de individuos de la muestra, cualquier conjunto de ese número de individuos tenga la misma probabilidad de ser seleccionado. 

Hay dos tipos  básicos de muestreo aleatorio que vale la pena distinguir. Para ilustrarlos, supongamos que disponemos de una urna con 100 bolas numeradas del 1 al 100, de la que queremos extraer una muestra de 15 bolas. La Figura siguiente representa dicha urna.

```{r base, echo=FALSE,fig.cap="Una urna de 100 bolas"}
knitr::include_graphics("basev.png")
```

Una manera de hacerlo sería repetir 15 veces el proceso de sacar una bola de la urna, anotar su número y devolverla a la urna. El tipo de muestra obtenida de esta manera recibe el nombre de **muestra aleatoria con reposición**, o **simple** (una **m.a.s.**, para abreviar). Observad que con este procedimiento una misma bola puede aparecer varias veces en una muestra, y que todos los subconjuntos de 15 bolas "con posibles repeticiones"  tienen la misma probabilidad de obtenerse. Un posible resultado serían las bolas azules de la Figura siguiente; la bola azul más oscuro ha sido escogida dos veces en la muestra.

```{r, echo=FALSE, label=simple,fig.cap="Una muestra aleatoria simple"}
knitr::include_graphics("simplev.png")
```

Para realizar un muestreo aleatorio en `R` con reposición hay que usar la función `sample`:

```{r}
sample(1:100, 15, replace=TRUE)
```
Fijémonos que las bolas elegidas no coinciden con las de la Figura. Esto es debido a que la función `sample` nos da cada vez una muestra diferente.

Otra manera de extraer nuestra muestra sería repetir 15 veces el proceso de sacar una bola de la urna  pero ahora sin devolverla. Esto es equivalente a extraer de golpe 15 bolas de la urna. Estas muestras no tienen bolas repetidas, y cualquier selección de 15 bolas diferentes tiene la misma probabilidad de ser la obtenida.  En este caso se habla de una **muestra aleatoria sin reposición**. Un posible resultado serían las bolas azules de la Figura siguiente:


```{r, echo=FALSE, label=sinrep,fig.cap="Una muestra aleatoria sin reposición"}
knitr::include_graphics("sinrepv.png")
```

Usando la función `sample` podemos extraer otro muestreo sin reposición:

```{r}
sample(1:100, 15, replace=FALSE)
```

Fijémonos que el parámetro `replace` controla si el muestreo es con reposición (`replace=TRUE`) o sin reposición (`replace=FALSE`).
Así, si queremos extraer una muestra aleatoria de 30 pacientes sin reposición de nuestra tabla de datos de pacientes, haríamos lo siguiente:
```{r}
library(tidyverse)
pacientes.muestra30=sample(1:165,30,replace=FALSE)
datos=read_tsv("../../datasets/segundo dataset/hcc_data.tsv")
datos.pacientes.muestra30 = datos[pacientes.muestra30,]
```
Recordemos que si queremos ver y trabajar con la nueva tabla de datos de la muestra de los 30 pacientes, podemos hacer `View(datos.pacientes.muestra30)` para visualizarlos en una tabla.

Si volvemos a ejecutar las funciones anteriores, nos saldrá otra muestra de 30 pacientes diferentes. La función de `R` que controla la aleatoriedad de las muestras obtenidas es la función `set.seed`. Si fijamos dicha función, obtendremos siempre la misma muestra de pacientes. Probad con
```{r}
set.seed(2019)
pacientes.muestra30=sample(1:165,30,replace=FALSE)
set.seed(2019)
pacientes.muestra30.2ointento=sample(1:165,30,replace=FALSE)
```
y comprobar que los pacientes elegidos en las dos muestras son los mismos.
Si queremos volver a "reiniciar" la semilla de la aleatoriedad tras haber usado un set.seed, podemos usar set.seed(NULL).

La función `replicate` nos permite realizar tareas más complejas relacionadas con el muestreo. Por ejemplo, imaginemos que
queremos calcular las medias de 10 muestras de 30 pacientes de la tasa de hemoglobina. Podemos hacer dicha tarea 
de la forma siguiente:

```{r}
set.seed(2000)  # fijamos la semilla
replicate(10,round(mean(sample(datos$`Haemoglobin (g/dL)`,30,replace=TRUE),na.rm=TRUE),3))
```


Cuando el tamaño de la población es muy grande en relación a la muestra, la probabilidad de que haya repeticiones en una muestra aleatoria simple es muy pequeña. Esto nos permite entender en este caso que los muestreos aleatorios con y sin reposición son equivalentes en el sentido siguiente: puesto que un muestreo con reposición da muy probablemente una muestra con todos sus elementos diferentes, aceptamos que una muestra obtenida sin reposición ha sido obtenida permitiendo repeticiones y que por tanto es simple. 

La mayoría de técnicas de estadística inferencial que se pueden usar para muestras aleatorias simples se pueden considerar igualmente válidas para muestras aleatorias sin reposición, siempre y cuando el tamaño de la población sea muy grande en relación al de la muestra (por dar una regla, al menos unas 1000 veces mayor).  Si el tamaño de la población es relativamente pequeño por comparación a la muestra, algunas de estas técnicas se pueden salvar aplicando correcciones adecuadas para compensar el efecto del tamaño de la población, y otras directamente pierden toda validez. 

En todo caso, conviene remarcar que si queremos tomar una muestra aleatoria con o sin reposición de una población, es necesario disponer de una lista completa de todos sus individuos  para poder sortear a quién vamos a seleccionar. Esto no siempre es posible. ¿Alguien tiene la  lista completa de, pongamos, todos los diabéticos de España? ¿Que incluya los que aún no saben que lo son? Por lo tanto, en la vida real no siempre podemos tomar muestras aleatorias en el sentido que hemos explicado.



#### Muestreo sistemático {-}

Una manera muy sencilla de obtener una muestra de una población cuando disponemos de una lista ordenada de sus individuos es tomarlos a intervalos constantes: cada quinto individuo, cada décimo individuo. Podemos añadir una componente aleatoria escogiendo al azar el primer individuo que elegimos, y a partir del cual empezamos a contar. Así, por ejemplo, si de la muestra de nuestra tabla de datos de 165 pacientes quisiéramos escoger una muestra de 30, podríamos elegir un paciente al azar, y a partir de él, por orden en la filas de la tabla de datos, elegir el paciente número 30, el paciente número 60, etc.; si al llegar al final de la lista de pacientes no hubiéramos completado la muestra, volveríamos al principio de la misma. A esta técnica se la llama **muestreo sistemático**, **aleatorio** si además el primer sujeto se escoge de manera aleatoria. Por ejemplo, la Figura siguiente describe una muestra aleatoria sistemática de 15 bolas de nuestra urna de 100 bolas: hemos empezado a escoger por la bola roja oscura, que ha sido elegida al azar, y a partir de ella hemos tomado 1 de cada 7 bolas, volviendo al principio cuando hemos llegado al final de la lista de bolas


```{r, echo=FALSE, label=sist,fig.cap="Una muestra aleatoria sistemática"}
knitr::include_graphics("sistv.png")
```

Cuando el orden de los individuos de la población en la lista es aleatorio, el muestreo sistemático aleatorio es equivalente al muestreo aleatorio sin reposición. Pero en general este no es el caso, y se pueden producir sesgos. 

```{r, include=FALSE}
options(digits=3)
```
   
Para realizar un muestreo sistemático en `R`, podemos usar la función siguente:
```{r}
sist.sample=function(N,n){
  k=ceiling(N/n)
  x0=sample(N,1)
  seq(x0,length.out=n,by=k)%%N
 }
```
donde `N` representa el tamaño de la población y `n`, el tamaño de la muestra a construir.

Por ejemplo, hagamos un muestreo sistemático de 30 pacientes usando la función anterior:
```{r}
(pacientes.sistemático.30 = sist.sample(165,30))
datos.sistemático.30 = datos[pacientes.sistemático.30,]
```

   
#### Muestreo aleatorio estratificado {-}

Este tipo de muestreo se utiliza cuando la población está clasificada en  **estratos** que son de interés para la propiedad estudiada. En este caso, se toma una muestra aleatoria de cada estrato y se unen en una muestra global. A este proceso se le llama **muestreo aleatorio estratificado**. Normalmente, se impone que la composición por estratos de la muestra global mantenga las proporciones de la población original; es decir, que el tamaño de la muestra de cada estrato represente el mismo porcentaje del total de la muestra que el estrato correspondiente en la población completa.  Por ejemplo, los estratos podrían ser grupos de edad, y entonces la muestra de cada grupo de edad se tomaría proporcional a la fracción que representa dicho grupo de edad en la población total. O podrían ser por sexos.

Por continuar con nuestra urna de 100 bolas, supongamos que contiene 40 bolas de un color y 60 de otro color según muestra la Figura siguiente:


```{r, echo=FALSE, label=estratprevi,fig.cap="Nuestra urna ahora tiene 2 estratos"}
knitr::include_graphics("estratprevi.png")
```
 

Para tomar una muestra aleatoria estratificada de 15 bolas, considerando como estratos los dos colores, tomaríamos una muestra aleatoria de 6 bolas del primer color y una muestra aleatoria de 9 bolas del segundo color. De esta manera, los porcentajes de colores en la muestra serían los mismos que en la urna. La Figura siguiente describe una muestra obtenida de esta manera.


```{r, echo=FALSE, label=estrat,fig.cap="Una muestra aleatoria estratificada"}
knitr::include_graphics("estrat.png")
```
 

En todo caso, el muestreo por estratos solo es necesario si esperamos que las características de la propiedad poblacional que queremos estudiar varíen según el estrato. Por ejemplo, si queremos tomar una muestra para estimar la tasa de hemoglobina de nuestra población de pacientes,  y no creemos que la tasa de hemoglobina dependa de la edad en que el paciente fue diagnosticado, no hay ninguna necesidad de esforzarse en tomar una muestra de cada estrato de edad de manera que todos los pacientes estén representadas proporcionalmente en la muestra.


#### Muestreo por conglomerados {-}

El proceso de obtener y estudiar una muestra aleatoria en algunos casos es caro o difícil, incluso aunque dispongamos de la lista completa de la población. Imaginemos que quisiéramos estudiar los hábitos de alimentación de los estudiantes de Primaria de Baleares.  Para ello, previo permiso de la autoridad competente, tendríamos que seleccionar una muestra representativa de los escolares de Baleares. Seguramente podríamos disponer de su lista completa y por lo tanto podríamos tomar una muestra aleatoria, pero entonces acceder a las niñas y niños que la formasen seguramente significaría  contactar con unos pocos alumnos de muchos centros de primaria, lo que volvería el proceso lento y costoso. Y eso si la *Conselleria d'Educació* nos facilitase la lista completa de alumnos.

Una alternativa posible sería, en vez de extraer una muestra aleatoria de todos los estudiantes de Primaria, escoger primero al azar unas pocas aulas de primaria de colegios de las Baleares, a las que llamamos en este contexto **conglomerados** (*clusters*), y formar entonces nuestra muestra con todos los alumnos de estas aulas. Y es que es mucho más sencillo poseer la lista completa de estudiantes de unas pocas aulas que conseguir la lista completa de todos los estudiantes de todos los colegios, y mucho más barato ir a unos pocos colegios concretos que ir a todos los colegios de las Islas a entrevistar a unos pocos estudiantes en cada centro. 

Efectuamos también un muestreo por conglomerados cuando para medir algunas características de los ejemplares de una planta en un bosque concreto, cuadriculamos la superficie del bosque, escogemos una muestra aleatoria de sectores de la cuadrícula (serían los conglomerados de este ejemplo) y estudiamos las plantas de interés contenidas en los sectores elegidas.

Volviendo de nuevo a nuestra urna, supongamos que sus 100 bolas se agrupan en 20 conglomerados de 5 bolas cada uno según las franjas verticales de la Figura siguiente (donde mantenemos la clasificación en dos colores para poder comparar el resultado del muestreo por conglomerados con el estratificado).


```{r, echo=FALSE, label=clustprevi,fig.cap="Nuestra urna ahora tiene 2 estratos y 20 conglomerados"}
knitr::include_graphics("clusterprevi.png")
```

Para obtener una muestra aleatoria por conglomerados de tamaño 15, escogeríamos al azar 3 conglomerados y la muestra estaría formada por sus bolas. 
 La Figura siguiente describe una muestra obtenida de esta manera: los conglomerados escogidos están marcados en azul.


```{r, echo=FALSE, label=clust,fig.cap="Una muestra aleatoria por conglomerados"}
knitr::include_graphics("cluster.png")
```


Observad la diferencia entre el muestreo estratificado y el muestreo por conglomerados:

* En una muestra **estratificada** se escoge una muestra aleatoria de cada estrato existente.

* En una muestra **por conglomerados** se escogen algunos conglomerados al azar y se incluye en la muestra todos sus elementos.


#### Muestreos no aleatorios {-}

Cuando la selección de la muestra no es aleatoria, se habla de  **muestreo no aleatorio**. En realidad es el tipo más frecuente de muestreo porque, en muchos casos,  nos tenemos que conformar con los sujetos disponibles.  Por ejemplo, en un centro educativo, para estimar la opinión que de un profesor tienen los alumnos de una clase, se consulta solo a los estudiantes que voluntariamente  rellenan la encuesta de opinión,  que de ninguna manera forman una  muestra aleatoria: el perfil del estudiante que contesta voluntariamente una encuesta de este tipo está muy definido y no viene determinado por el azar. En este caso se trataría de una **muestra autoseleccionada**. 

Otro tipo de muestreos no aleatorios son los **oportunistas**. Este es el caso, por ejemplo, si para estimar la opinión que de un profesor tienen los alumnos de una asignatura se visita un día la clase y se pasa la encuesta a los estudiantes que ese día asistieron a clase.

La Figura siguiente describe una muestra oportunista de nuestra urna: sus 15 primeras bolas. Aunque toda muestra de un mismo tamaño tiene la misma probabilidad de obtenerse por medio de un muestreo aleatorio sin reposición, es difícil de creer que esta muestra sea aleatoria; basta que calculéis cuál es la probabilidad de que en una muestra aleatoria de 15 bolas de nuestra urna todas tengan el mismo color:

```{r,include=FALSE}
options(scipen=999)
```


$$
\frac{40\times 39\times \cdots\times 26+60\times 59\times \cdots\times 46}{100\times 99\times \cdots\times 86}\approx `r round((prod(26:40)+prod(46:60))/prod(86:100),5)`
$$


```{r,include=FALSE}
options(scipen=0)
```


```{r, echo=FALSE, label=oport,fig.cap="Una muestra oportunista"}
knitr::include_graphics("oport.png")
```


#### Muestreo polietápico {-}

En el ejemplo de los estudiantes de Primaria, la muestra final de estudiantes ha estado formada por todos los individuos de las aulas elegidas. Otra opción podría haber sido, tras seleccionar la muestra aleatoria de conglomerados, tomar de alguna manera una muestra aleatoria de cada uno de ellos. Por ejemplo, algunos estudios poblacionales a nivel estatal se realizan solamente en algunas provincias escogidas aleatoriamente, en las que luego se encuesta una muestra aleatoria de habitantes. Este sería un ejemplo de **muestreo polietápico**, en el que la muestra no se obtiene en un solo paso, sino mediante diversas elecciones sucesivas. La Figura siguiente muestra un ejemplo sencillo de muestreo polietápico de nuestra urna: hemos elegido al azar 5 conglomerados (marcados en azul) y de cada uno de ellos hemos elegido 3 bolas al azar sin reposición.


```{r, echo=FALSE, label=poli,fig.cap="Una muestra polietápica"}
knitr::include_graphics("poli.png")
```

Existen otros tipos de muestreo, solo hemos explicado los más comunes. En cualquier caso, lo importante es recordar que el estudio estadístico que se realice *a posteriori* deberá ser diferente según el muestreo usado. Por ejemplo, no se pueden usar las mismas técnicas para analizar una muestra aleatoria simple que una muestra por conglomerados. 


## Guía rápida

* `sample(x, n, replace=...)` genera una muestra aleatoria de tamaño `n` del vector `x`, con reposición si igualamos `replace` a `TRUE` y sin reposición si lo igualamos a `FALSE` (su valor por defecto). Si `x` es un número natural $x$, representa el vector 1,2,...,$x$.

* `set.seed` permite fijar la semilla de aleatoriedad. 

* `replicate(n,expresión)` evalúa `n` veces la `expresión`, y organiza los resultados como las columnas de una matriz (o un vector, si el resultado de cada `expresión` es unidimensional).


## Estimación puntual

En un estudio inferencial, una vez tomada la muestra y obtenidos los datos sobre sus miembros, el siguiente paso es inferir, es decir, deducir información sobre la población a partir de estos datos. Dicha información se puede deducir de dos formas:

* Suponiendo que conocemos el **modelo** al que se ajusta la población: es decir, suponiendo que conocemos el tipo de distribución de la variable aleatoria que modela la característica de la población en la que estamos interesados, pero desconocemos uno o varios parámetros de los que depende dicha distribución. Así, podemos saber (o suponer) que las longitudes de los ejemplares adultos de una cierta especie se distribuyen según una variable aleatoria normal, pero desconocer sus parámetros $\mu$ (media) y $\sigma$ (desviación típica), y usar este conocimiento para inferir información sobre dichas longitudes a partir de las de una muestra: por ejemplo, para estimar con un cierto margen de error su longitud media. Si estamos en este caso, hablaremos de **estimación paramétrica**.

* Suponiendo que desconocemos qué tipo de distribución tiene la variable aleatoria que modela la característica que nos interesa (aunque a veces necesitaremos saber algo de esta distribución; por ejemplo, si es simétrica o no). En este caso, hablaremos de **estimación no paramétrica**.

En ambos casos, existen tres vías para obtener información sobre los parámetros de la distribución (conocida o desconocida) de la variable aleatoria que nos interesa:

* **Estimación puntual**. Se trata de obtener expresiones matemáticas, llamadas **estimadores puntuales**, que aplicadas a los valores de una muestra nos dan una aproximación (el término exacto es una **estimación**) del valor de dicho parámetro para la población. A modo de ejemplo, la media aritmética de los datos $x_1,\ldots,x_n$ de una muestra,
$\overline{x}=\frac{x_1+\cdots +x_n}{n},$
es un estimador del **valor medio** (**valor esperado**, **esperanza**) de la variable aleatoria de la que hemos extraído la muestra.

* **Estimación por intervalos de confianza**. Se trata de obtener intervalos que contengan con probabilidad alta el parámetro objeto de estudio.

* **Contraste de hipótesis**. *Grosso modo*, se establecen dos hipótesis opuestas sobre el parámetro o, más en general, sobre la distribución de la variable aleatoria, y se contrastan para intentar decidir cuál es la verdadera. 


En esta sección hablaremos de la estimación puntual. Para empezar, es obvio que no toda fórmula matemática sirve para estimar de manera sensata el valor de un parámetro. Por ejemplo, si queremos estimar la tasa media de bilirrubina total de una muestra de nuestra tabla de datos, calcularemos la media de las tasas de bilirrubina en la muestra y dar ese valor como estimación de la tasa de bilirrubina total de todos los pacientes de la tabla de datos. Y es lo correcto, porque la media muestral es siempre un estimador **insesgado** de la media poblacional y muy a menudo es además su estimador **máximo verosímil**. Veamos qué significan estas propiedades.

* **Insesgado**: Los valores de un estimador sobre muestras aleatorias de una población forman una variable aleatoria con una distribución de probabilidad propia, llamada genéricamente  **muestral**. Decimos entonces que un estimador es **insesgado** cuando el valor esperado de la variable aleatoria que define coincide con el valor del parámetro poblacional que se quiere estimar. Por ejemplo, si se toman muestras aleatorias con o sin reposición, la media muestral es siempre un estimador insesgado del valor medio poblacional: su valor esperado es el valor medio poblacional.

* **Máximo verosímil**: Cada muestra aleatoria de una población tiene una probabilidad de obtenerse que no solo depende de la muestra, sino también de la distribución de probabilidad de la variable aleatoria poblacional. Si la distribución poblacional es de un tipo concreto (Bernoulli, normal, ...), esta probabilidad depende de sus parámetros. Decimos entonces que un estimador es **máximo verosímil** cuando el resultado que da sobre cada muestra aleatoria es el valor del parámetro poblacional que maximiza la probabilidad de obtenerla. Por ejemplo, si lanzamos una moneda al aire $n$ veces y calculamos la proporción de veces que obtenemos cara, esa **proporción muestral** $\widehat{p}$ es el estimador máximo verosímil de la probabilidad $p$ de obtener cara con esa moneda. Esto quiere decir que, de entre todas las distribuciones binomiales $B(n,p)$ que pueden modelar el número de caras que obtenemos al lanzar $n$ veces nuestra moneda, aquella que asigna mayor probabilidad al número de caras que hemos obtenido es la que tiene como parámetro $p$ la frecuencia relativa de caras $\widehat{p}$ que hemos observado.

Para algunas distribuciones, el método de máxima verosimilitud de estimación de sus parámetros da lugar a fórmulas cerradas más o menos sencillas, pero en otros casos nos tenemos que conformar con un valor aproximado obtenido mediante algún método numérico.


### Estimación máximo verosímil


A continuación recordamos una lista de los estimadores máximo verosímiles de los parámetros de las distribuciones más comunes a partir de una muestra aleatoria simple:

* Para la familia Bernoulli, el estimador máximo verosímil del parámetro $p$ es la proporción muestral de éxitos $\widehat{p}$. Este estimador es además insesgado.

*  Para la familia Poisson, el estimador máximo verosímil  del parámetro $\lambda$ es la media muestral $\overline{X}$. Este estimador es de nuevo  insesgado.

*  Para la familia geométrica, el estimador máximo verosímil del parámetro $p$ es ${1}/{\overline{X}}$. Este estimador es sesgado.

*   Para la familia exponencial, el estimador máximo verosímil del parámetro $\lambda$ es ${1}/{\overline{X}}$. Este estimador también es sesgado.

*  Para la familia normal, los  estimadores máximo verosímiles de la media $\mu$, la desviación típica $\sigma$ y la varianza $\sigma^2$ son, respectivamente, la media muestral $\overline{X}$, la desviación típica "verdadera" $S_X$ y la varianza "verdadera" $S_X^2$. Además, $\overline{X}$ es un estimador insesgado de $\mu$. La varianza verdadera $S_X^2$ no es un estimador insesgado de  $\sigma^2$, pero sí que lo es la varianza muestral $\widetilde{S}^2$. Y ninguna de las dos desviaciones típicas, ni la  "verdadera" $S_X$ ni la muestral $\widetilde{S}_X$, es un estimador  insesgado de $\sigma$; si necesitáis un estimador insesgado de la desviación típica de una variable aleatoria normal a partir de una muestra aleatoria simple, lo podéis encontrar en la [correspondiente entrada de la Wikipedia](http://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation). No obstante, el beneficio de usar este estimador insesgado no suele compensar lo complicado de su cálculo.


Cuando se estima algún parámetro de una distribución a partir de una muestra, es conveniente aportar el **error típico**, o **estándar**,  como medida de la finura de la estimación. Recordemos que el **error típico de un estimador** es la desviación típica de su distribución muestral, y que el **error típico de una estimación** a partir de una muestra es la estimación del error típico del estimador usando dicha muestra. Como veremos en la próxima lección, estos errores típicos serán una ingrediente clave en el cálculo de intervalos de confianza.

Veamos un ejemplo sencillo.  Supongamos que tenemos una muestra aleatoria simple de tamaño $n$ de una variable $X$ que sigue una distribución Bernoulli de probabilidad poblacional $p$ desconocida que queremos estimar. Por ejemplo, puede ser que tengamos una moneda posiblemente trucada, la hayamos lanzado 100 veces al aire y hayamos anotado los resultados (1, cara, 0, cruz), y a partir de este experimento queramos estimar la probabilidad de sacar cara con esta moneda. O que hayamos anotado para 100 individuos de una población  elegidos al azar si tienen o no una determinada enfermedad (1 significa que sí, 0 que no) y a partir de esta muestra deseemos estimar la **prevalencia** de la enfermedad en la población, es decir, la proporción real de enfermos, que coincide con la probabilidad de que un individuo elegido al azar tenga dicha enfermedad. Tomemos, para fijar ideas, la siguiente muestra de tamaño 100:

```{r}
x=c(0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0,
0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,
1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0)
```

En este caso, podemos estimar $p$ mediante la proporción muestral de éxitos $\widehat{p}$, que coincide con la media muestral. El error típico de este estimador es $\sqrt{p(1-p)/n}$, y el error típico de una estimación concreta es $\sqrt{\widehat{p}(1-\widehat{p})/n}$. Por lo tanto, a mano podemos estimar $p$ y calcular el error típico de dicha estimación de la manera siguiente:

```{r}
n=length(x)  #Tamaño de la muestra
estim.p=mean(x)  #Proporción muestral
estim.p
error.tip.p=sqrt(estim.p*(1-estim.p)/n) #Error típico de la estimación
error.tip.p
```

De esta manera, estimamos que $p$=`r estim.p`  con un error típico de `r round(error.tip.p,2)`.




Con R podemos estimar un parámetro de una distribución por el método de máxima verosimilitud a partir de una muestra y además obtener el error típico de dicha estimación usando la función `fitdistr` del paquete **MASS**.  Esta función calcula los estimadores máximo verosímiles de los parámetros de la mayoría de las familias de distribuciones disponibles en R.

Su  sintaxis básica es

```{r, eval=FALSE}
fitdistr(x, densfun=..., start=...)
```

donde

*  `x` es la muestra, un vector numérico.

*  El valor de `densfun` ha de ser el nombre de la familia de distribuciones; se tiene que entrar  entre comillas y puede tomar, entre otros, los valores siguientes: `"chi-squared"`, `"exponential"`, `"f"`, `"geometric"`,  `"lognormal"`,  `"normal"` y `"poisson"`.  La lista de distribuciones a las que se puede aplicar, que podéis consultar en la Ayuda de la función, no incluye la Bernoulli ni la binomial.

*  Si `fitdistr` no dispone de una fórmula cerrada para el estimador  máximo verosímil de algún parámetro, usa un algoritmo numérico para aproximarlo que requiere de un valor inicial para arrancar. Este valor (o valores) se puede especificar igualando el parámetro `start` a una `list` con cada parámetro a estimar igualado a un valor inicial.  Para algunas distribuciones, como la `"t"`,  `fitdistr` sabe tomar valores iniciales razonables, y no es necesario especificar el parámetro `start`. Pero para otras distribuciones, como por ejemplo la `"chi-squared"`, es obligatorio especificarlo. Para las distribuciones que disponen de fórmula cerrada, como la `"normal"` o la  `"poisson"`,  se tiene que omitir el parámetro `start`. 

Como no podemos usar `fitdistr` para estimar el parámetro $p$ de una Bernoulli (los autores del paquete debieron de considerar que era más fácil estimarlo directamente), vamos a usarla en otro ejemplo. Consideremos la siguiente muestra `y` de 100 valores generados con distribución de Poisson de parámetro $\lambda=10$:

```{r}
set.seed(100) 
y=rpois(100,10)
set.seed(NULL) 
y
```

Vamos a estimar el parámetro $\lambda$ de una distribución Poisson que haya generado este vector:

```{r}
library(MASS)
fitdistr(y, densfun="poisson")
```

El resultado dice que el valor estimado de  $\lambda$ es `r round(fitdistr(y, densfun="poisson")$estimate, 2)`, con un error típico en esta estimación de   `r round(fitdistr(y, densfun="poisson")$sd, 2)`. Veámoslo directamente:  el estimador máximo verosímil de $\lambda$ es la media aritmética $\overline{X}$ y el error típico de este estimador es $\sqrt{\lambda}/\sqrt{n}$ (recordad que la desviación típica de una Poisson de parámetro $\lambda$ es $\sqrt{\lambda}$ y que el error típico de la media muestral es la desviación típica poblacional dividida por la raíz cuadrada del tamaño de la muestra), por lo que el error típico de una estimación es  $\sqrt{\overline{X}}/\sqrt{n}$.

```{r}
mean(y)
sqrt(mean(y)/length(y))
```

También podemos  estimar la media y la desviación típica de una variable normal que hubiera producido esta muestra.

```{r}
fitdistr(y, densfun="normal")
```

Observad que la estimación de la desviación típica que nos da `fitdistr` es la desviación típica "verdadera" (que es su estimador máximo verosímil) y no la muestral:

```{r}
sd(y)
sqrt((length(y)-1)/length(y))*sd(y)
```

Vamos a estimar ahora el número de grados de libertad de una t de Student que hubiera producido esta muestra.

```{r}
fitdistr(y, densfun="t")
```


El resultado de  `fitdistr` es una `list`, y por lo tanto el
 valor de cada estimador y su error típico se pueden obtener con los sufijos adecuados. En concreto, los valores estimados forman la componente `estimate` y los errores típicos la componente `sd`. Para obtenerlos directamente, basta usar los sufijos `$estimate` y `$sd`, respectivamente:
 
```{r}
fitdistr(y,"poisson")$estimate  #Estimación de lambda
fitdistr(y,"poisson")$sd   #Error típico
fitdistr(y,"normal")$estimate  #Estimaciones 
fitdistr(y,"normal")$estimate[1]  #Estimación de mu
fitdistr(y,"normal")$estimate[2]  #Estimación de sigma
```


###  Guía rápida

*  `fitdistr` del paquete **MASS**, sirve para calcular los estimadores  máximo verosímiles  de los parámetros de una distribución a partir de una muestra. El resultado es una `list` que incluye los objetos `estimate` (los valores estimados) y `sd` (los errores típicos de las estimaciones). Sus parámetros principales son:

     * `densfun`: el nombre de  la familia de distribuciones, entre comillas.
     * `start`: permite fijar el valor inicial del algoritmo numérico para calcular el estimador, si la función lo requiere.



### Ejercicios


*(1)* Con una sola  instrucción, calculad la media del nivel de leucocitos de una muestra aleatoria sin reposición de 15 pacientes. Escribir `set.seed(2019)` antes de ejecutar la instrucción

Solución:

```{r}
set.seed(2919)
mean(sample(datos$`Leukocytes(G/L)`,15,replace = FALSE))
```


*(2)* Con una sola instrucción, extraed una subtabla de datos de la tabla de datos de los pacientes formada por una muestra aleatoria sin reposición de 40 pacientes, y llamadlo `muestra`. Escribir `set.seed(2019)` antes de ejecutar la instrucción.

Solución:
```{r}
set.seed(2019)
muestra=datos[sample(dim(datos)[1],40,replace=FALSE),]
```

*(3)* Con una sola instrucción, calculad un vector formado por las medias de 100 muestras aleatorias sin reposición del nivel de Ferritina de  20 pacientes (cuidado con los NA's) y llamadlo medias. Escribir `set.seed(2019)` antes de ejecutar la instrucción.

Solución:
```{r}
set.seed(2019)
medias=replicate(100,mean(sample(datos$`Ferritin (ng/mL)`,20),na.rm=TRUE))
medias
```


*(4)* Las distribuciones de Weibull tienen dos parámetros, forma, `shape`, y escala, `scale`.  Supongamos que los datos siguientes siguen una distribución de Weibull: 2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72. Calculad el estimador máximo verosímil del parámetro de escala de esta distribución, redondeado a 3 cifras decimales. Tenéis que dar el resultado, no cómo lo habéis calculado. 

Solución:
```{r}
muestra.weibull=c(2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72)
library(MASS)
fitdistr(muestra.weibull,"weibull")
```
El valor pedido es: `r round(fitdistr(muestra.weibull,"weibull")$estimate[2],3)`.

*(5)* Generad, con semilla de aleatoriedad igual a 42, una secuencia aleatoria de 100 valores con distribución geométrica Ge(0.6). A continuación estimad por máxima verosimilitud el parámetro $p$ de una distribución geométrica que haya generado dicha muestra y dad como respuesta a esta pregunta *el error típico de esta estimación* redondeado a 3 cifras decimales.

Solución:

```{r}
set.seed(42)
muestra.geométrica=rgeom(100,0.6)
fitdistr(muestra.geométrica,"geometric")
```
El valor pedido es: `r round(fitdistr(muestra.geométrica,"geometric")$sd,3)`.


Cuestionario:

1)  Queremos escoger 100 estudiantes de grado de una cierta universidad para preguntarles cuántas horas semanales estudian. Como creemos que el tipo de estudio cursado influye en este dato, clasificamos los estudiantes según el centro (facultad o escuela) en el que están matriculados, y tomaremos una muestra al azar de cada centro, por sorteo a partir de la lista de todos los matriculados en ese centro y de manera que el tamaño de la muestra de cada centro sea proporcional al número de matriculados en el mismo. ¿De qué tipo de muestreo se tratará?

    (a) Muestreo aleatorio simple
    (b) Muestreo aleatorio estratificado (respuesta correcta)
    (c) Muestreo aleatorio sin reposición
    (d) Muestreo aleatorio por conglomerados
    (e) Ninguna respuesta excepto ésta es correcta
    
2) Consideremos la tabla de datos `iris` que tiene 50 flores de tres especies distintas: `setosa`, `versicolor` y `virgínica`. Dicha tabla de datos tiene 5 variables: la longitud (`Sepal.Lenght`) y amplitud (`Sepal.Width`) del sépalo, la longitud (`Petal.Lenght`) y amplitud (`Petal.Width`) del sépalo y la especie de la flor. Calculamos las medias de 80 muestras con reposición de la longitud del pétalo de tamaño 30. ¿Cuánto vale la media de estas 80 medias redondeada a 3 cifras decimales si hacemos `set.seed(2019)` antes de calcular dichas medias?

    a) 3.716  (respuesta correcta)
    b) 4.531
    c) 2.954
    d) 2.666
    e) Ninguna respuesta excepto ésta es correcta
    
3) Consideremos la tabla de datos `iris` que tiene 50 flores de tres especies distintas: `setosa`, `versicolor` y `virgínica`. Dicha tabla de datos tiene 5 variables: la longitud (`Sepal.Lenght`) y amplitud (`Sepal.Width`) del sépalo, la longitud (`Petal.Lenght`) y amplitud (`Petal.Width`) del sépalo y la especie de la flor. Supongamos que la amplitud del sépalo de las flores de la especie setosa es una muestra obtenida a partir de una distribución normal. ¿Cúal es el valor máximo verosímil de la desviación típica redondeado a tres cifras decimales?

    a) 0.432
    b) `r round(fitdistr(iris$Sepal.Width[iris$Species=="setosa"],"normal")$estimate[2],3)` (respuesta correcta)
    c) 0.201
    d) 0.667
    e) Ninguna respuesta excepto ésta es correcta
    
4) Generamos una muestra aleatoria de exponencial de parámetro $\lambda =5$ de tamaño 50. Hallar el error típico del estimador máximo verosímil de $\lambda$ redondeado a tres cifras decimales. Escribir `set.seed(2019)` antes de ejecutar la instrucción correspondiente.

    a) 0.102
    b) 0.339
    c) 0.566 (respuesta correcta)
    d) 0.881
    e) Ninguna respuesta excepto ésta es correcta
    

## Referencias

* [AprendeRII](https://github.com/cescrossello/AprendeR-II). Cursos de AprendeRII team (DMI UIB)
* [Github del curso de Estadística Descriptiva con R y Python](https://github.com/joanby/r-basic) Juan Gabriel Gomila.
* [Github del curso de R para Ciencia de Datos](https://github.com/joanby/tidyverse-data-science) Juan Gabriel Gomila.



