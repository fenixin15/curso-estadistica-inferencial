---
title: "Tema 6 - Estimación Puntual"
author: "Juan Gabriel Gomila, Arnau Mir y Ricardo Alberich"
date: 
output: 
  ioslides_presentation:
    widescreen: true
    css: Mery_style.css
    logo: Images/matriz_mov.gif
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Estimación puntual

* Estudio inferencial: deducir información sobre la población a partir de los datos de la muestra. Dos formas:
  * Suponiendo que conocemos el **modelo** al que se ajusta la población, es decir, suponiendo que conocemos el tipo de distribución de la variable aleatoria que modela la característica de la población en la que estamos interesados, pero desconocemos uno o varios parámetros de los que depende dicha distribución: **estimación paramétrica**.
  * Suponiendo que desconocemos qué tipo de distribución tiene la variable aleatoria que modela la característica que nos interesa: **estimación no paramétrica**.

## Estimación puntual

Tres vías para obtener información sobre los parámetros de la distribución:

* **Estimación puntual**. Se trata de obtener expresiones matemáticas, llamadas **estimadores puntuales**, que aplicadas a los valores de una muestra nos dan una aproximación (el término exacto es una **estimación**) del valor de dicho parámetro para la población. 
<div class="example">
Ejemplo: dada la muestra $x_1,\ldots,x_n$, la **media aritmética** 
  $$
  \overline{x}=\frac{x_1+\cdots +x_n}{n},
  $$
  es un estimador del valor medio de la población.
</div>
## Estimación puntual
Otras vías:

* **Estimación por intervalos de confianza**. Se trata de obtener intervalos que contengan con probabilidad alta el parámetro objeto de estudio. 
  
* **Contraste de hipótesis**. *Grosso modo*, se establecen dos hipótesis opuestas sobre el parámetro o, más en general, sobre la distribución de la variable aleatoria, y se contrastan para intentar decidir cuál es la verdadera. 

## Estimación puntual
Propiedades de un estimador:

* **Insesgado**: Los valores de un **estimador** sobre muestras aleatorias de una población forman una variable aleatoria con una distribución de probabilidad propia, llamada genéricamente **muestral**. Un estimador es **insesgado** cuando el valor esperado de la variable aleatoria que define coincide con el valor del parámetro poblacional que se quiere estimar.
<div class="example">
Ejemplo: si se toman muestras aleatorias con o sin reposición, la media muestral es siempre un estimador insesgado del valor medio poblacional: su valor esperado es el valor medio poblacional.
</div>
## Estimación puntual
Más propiedades:

* **Máximo verosímil**: Cada muestra aleatoria de una población tiene una probabilidad de obtenerse que no solo depende de la muestra, sino también de la distribución de probabilidad de la variable aleatoria poblacional. Un estimador es **máximo verosímil** cuando el resultado que da sobre cada muestra aleatoria es el valor del parámetro poblacional que maximiza la probabilidad de obtenerla.
<div class="example">
Ejemplo: Si lanzamos una moneda al aire $n$ veces y calculamos la proporción de veces que obtenemos cara, esa proporción muestral $\hat{p}$ es el estimador máximo verosímil de la probabilidad $p$ de obtener cara con esa moneda.
</div>

## Estimación máximo verosímil
Estimadores máximo verosímil más usuales:

* Para la familia Bernoulli, el estimador máximo verosímil del parámetro $p$ es la proporción muestral de éxitos $\widehat{p}$. Este estimador es además insesgado.

* Para la familia Poisson, el estimador máximo verosímil  del parámetro $\lambda$ es la media muestral $\overline{X}$. Este estimador es de nuevo  insesgado.

* Para la familia geométrica, el estimador máximo verosímil del parámetro $p$ es ${1}/{\overline{X}}$. Este estimador es sesgado.

## Estimación máximo verosímil

* Para la familia exponencial, el estimador máximo verosímil del parámetro $\lambda$ es ${1}/{\overline{X}}$. Este estimador también es sesgado.

* Para la familia normal, los  estimadores máximo verosímiles de la media $\mu$, la desviación típica $\sigma$ y la varianza $\sigma^2$ son, respectivamente, la media muestral $\overline{X}$, la desviación típica "verdadera" $S_X$ y la varianza "verdadera" $S_X^2$. La varianza verdadera $S^2_X$ no es un estimador insesgado de $\sigma^2$, pero sí que lo es la varianza muestral $\tilde{S}^2$.

## Error típico de un estimador
Cuando se estima algún **parámetro** de una distribución a partir de una muestra, es conveniente aportar el **error típico**, o estándar, como medida de la finura de la estimación. 

Recordemos que el **error típico** de un **estimador** es la desviación típica de su distribución muestral, y que el error típico de una estimación a partir de una muestra es la estimación del error típico del estimador usando dicha muestra.

## Error típico de un estimador
<div class="example">
Ejemplo: consideremos que hemos lanzado una moneda 25 veces y hemos obtenido los resultados siguientes:
```{r}
set.seed(2019)
resultados=sample(c("cara","cruz"),25,replace=TRUE)
```
La estimación de la probabilidad de obtener cara $p$ es la proporción muestral, que vale el cociente entre el número de veces que hemos obtenido cara (`r table(resultados)[1]`) y el número de lanzamientos (25):
```{r}
(prop.muestral = table(resultados)[1]/25)
```

</div>

## Error típico de un estimador
<div class="example">
El error típico del estimador $\hat{p}$ vale $\sqrt{\frac{p\cdot (1-p)}{n}}$, donde $n$ es el número de lanzamientos. 

El error típico de nuestra estimación concreta será: $\sqrt{\frac{\hat{p}\cdot (1-\hat{p})}{n}}$:
```{r}
sqrt(prop.muestral*(1-prop.muestral)/25)
```

</div>
## Estimación puntual con `R`

Para obtener estimaciones puntuales con `R` hay que usar la función `fitdistr` del paquete **MASS**:
```{r, eval=FALSE}
fitdistr(x, densfun=..., start=...)
```
donde

  *  `x` es la muestra, un vector numérico.

  *  El valor de `densfun` ha de ser el nombre de la familia de distribuciones:`"chi-squared"`, `"exponential"`, `"f"`, `"geometric"`,  `"lognormal"`,  `"normal"` y `"poisson"`. 

## Estimación puntual con `R`
  *  Si `fitdistr` no dispone de una fórmula cerrada para el estimador  máximo verosímil de algún parámetro, usa un algoritmo numérico para aproximarlo que requiere de un valor inicial para arrancar. Este valor (o valores) se puede especificar igualando el parámetro `start` a una `list` con cada parámetro a estimar igualado a un valor inicial.  

## Ejemplos de uso de `fitdistr`

<div class="example">
Estimación del parámetro $\lambda$ de una variable de Poisson:

En primer lugar generamos una muestra de tamaño~$50$ de una variable de Poisson de parámetro $\lambda =5$:
```{r}
set.seed(98)
muestra.poisson = rpois(50,lambda=5)
muestra.poisson
```

</div>

## Ejemplos de uso de `fitdistr`
<div class="example">
Vamos a estimar el valor del parámetro $\lambda$ a partir de la muestra anterior:
```{r}
library(MASS)
fitdistr(muestra.poisson, densfun = "poisson")
```
La función `fitdistr` nos ha dado el siguiente valor de $\lambda$: `r fitdistr(muestra.poisson, densfun = "poisson")[[1]]`, valor que se aproxima al valor real de $\lambda =5$, con un error típico de `r fitdistr(muestra.poisson, densfun = "poisson")[[2]]`.
</div>

## Ejemplos de uso de `fitdistr`
<div class="example">
Recordemos que el estimador máximo verosímil de $\lambda$ es $\overline{X}$ con error típico $\frac{\sqrt{\lambda}}{\sqrt{n}}$. Veamos si la función `fitdistr` nos ha mentido:
```{r}
(estimación.lambda = mean(muestra.poisson))
(estimación.error.típico= sqrt(estimación.lambda/50))
```
Comprobamos que los valores anteriores coinciden con los dados por la función.
</div>

## Ejemplos de uso de `fitdistr`


<div class="example">
¿Qué estimaciones hubiésemos obtenido de la media $\mu$ y la desviación típica $\sigma$ si suponemos que la muestra anterior es normal?

```{r}
fitdistr(muestra.poisson,densfun = "normal")
```
Dichos valores coinciden con la media muestral $\overline{X}$ y la desviación típica "verdadera" de la muestra considerada:
```{r}
sd(muestra.poisson)*sqrt(49/50)
```

</div>



## Guía rápida

*  `fitdistr` del paquete **MASS**, sirve para calcular los estimadores  máximo verosímiles  de los parámetros de una distribución a partir de una muestra. Parámetros principales:
    * `densfun`: el nombre de  la familia de distribuciones, entre comillas.
    * `start`: permite fijar el valor inicial del algoritmo numérico para calcular el estimador, si la función lo requiere.