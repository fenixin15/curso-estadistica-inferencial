---
title: "Tema 3 - Distribuciones Notables"
author: "Ricardo Alberich, Juan Gabriel Gomila y  Arnau Mir"
date: 
runtime: shiny
output: 
  ioslides_presentation:
    widescreen: true
    css: Mery_style.css
    logo: Images/matriz_mov.gif
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Distribuciones Notables. 

## Introducción

* En este tema estudiaremos diversos tipos de experimentos que son muy frecuentes y algunas de las variables aleatorias asociadas a ellos. 

* Estas variables reciben distintos nombres que aplicaremos sin distinción al tipo de población del experimento a la variable o a su función de probabilidad, densidad o distribución.

* Empezaremos con las variables aleatorias discretas que se presentan con frecuencia ya que están relacionadas con situaciones muy comunes como el número de caras en varios lanzamiento de una moneda, el número de veces que una maquina funciona hasta que se estropea, el numero de clientes en una cola,...


# Distribuciones Discretas. 

## Distribución Bernoulli


<l class="definition"> Distribución Bernouilli </l>

* Consideremos un experimento con dos resultados posibles éxito (E) y
fracaso (F). El espacio de sucesos será $\Omega=\{E,F\}$.
* Supongamos que la probabilidad de éxito es  $P(E)=p$,  y naturalmente $P(F)=1-p=q$ con $0<p<1$.
* Consideremos la  aplicación 

$$
X:\Omega=\{E,F\}\to \mathbb{R}$$

definida por

$$X(E)=1\mbox{, }X(F)=0$$


## Distribución Bernoulli
* Su  función de probabilidad es


$$
P_{X}(x)=
\left\{
\begin{array}{ll} q & \mbox{si } x=0\\
p & \mbox{si } x=1\\
0 & \mbox{en cualquier otro caso}
\end{array}
\right.
$$

* Su función de distribución es 

$$
F_{X}(x)=P(X\leq x)
\left\{
\begin{array}{ll} 
0 & \mbox{si } x<0\\

1-p & \mbox{si } 0\geq x <1\\
1 & \mbox{si } 1\geq 1 \\
\end{array}
\right.
$$


## Distribución Bernoulli

* Bajo estas condiciones diremos que $X$  **es una v.a. Bernoulli** o que  sigue una ley de  **distribución de probabilidad  Bernoulli** de parámetro $p$.
* Lo denotaremos por
$$X\equiv Ber(p)\mbox{ o también } X\equiv B(1,p).$$
* A este tipo de  experimentos (éxito/fracaso)se les denomina experimentos Bernoulli.

* Fue su descubridor un científico suizo  [Jacob Bernoulli](https://es.wikipedia.org/wiki/Jakob_Bernoulli),  uno más de la de la conocida [familia de científicos suizos Bernoulli](https://es.wikipedia.org/wiki/Familia_Bernoulli)

## Esperanza de una v.a. $X$ con distribución $Ber(p)$·

Su **valor esperado** es 

$$E(X)=\displaystyle\sum_{i=1}^2 x\cdot P(X=x)= 0\cdot(1-p)+1\cdot(p)=p.$$



Calculemos también $E(X^2)$

$$E(X^2)=\displaystyle\sum_{i=1}^2 x^\cdot P(X=x)= 0^2\cdot(1-p)+1^2\cdot(p)=p.$$

## Varianza de una v.a. $X$ con distribución $Ber(p)$·


Su  **varianza** es 

$$Var(X)=E(X^2)-\left(E(X)\right)^2=p+p^2=p\cdot (1-p).$$


Su desviación típica es 

$$\sqrt{Var(X)}=\sqrt{p\cdot (1-p)}.$$

## Resumen v.a con distribución Bernoulli, $Ber(p)$


$X$  Bernoulli | $Ber(p)$
-----------:|:--------
$D_X=$ | $\{0,1\}$
$P_X(x)=P(X=x)=$ |  $\left\{\begin{array}{ll} q & \mbox{si } x=0\\ p & \mbox{si } x=1\\0 & \mbox{en otro caso}\end{array}\right.$ 
$F_X(x)=P(X\leq X)=$ | $\left\{\begin{array}{ll} 0 & \mbox{ si } x<0\\q & \mbox{ si } 0\leq x<1\\1 & \mbox{ si } 1\leq x \end{array}\right.$
$E(X)=p$ | $Var(X)=p\cdot q$



## Veamos los cálculos básicos $Ber(p=0.25)$

```{r binomialfunciones}
dbinom(0,size=1,prob=0.25)
dbinom(1,size=1,prob=0.25)
rbinom(n=20,size = 1,prob=0.25)
```





## Gráficos de la  distribución   $Ber(p=0.25)$


El siguiente código dibuja las función de probabilidad y la de distribución de una  $Ber(p=0.25)$

```{r eval=FALSE}
plot(x=c(0,1),y=dbinom(c(0,1),size=1,prob=0.25),
    ylim=c(0,1),xlim=c(-1,2),xlab="x",
    main="Función de probabilidad\n Ber(p=0.25)")
lines(x=c(0,0,1,1),y=c(0,0.75,0,0.25), type = "h", lty = 2,col="blue")
curve(pbinom(x,size=1,prob=0.25),
    xlim=c(-1,2),col="blue",
    main="Función de distribución\n Ber(p=0.25)")
```


## Gráficos de la  distribución  $Ber(p=0.25)$

```{r fig.align='center',echo=FALSE}
par(mfrow=c(1,2))
plot(x=c(0,1),y=dbinom(c(0,1),size=1,prob=0.25),
     ylim=c(0,1),xlim=c(-1,2),xlab="x",
     main="Función de probabilidad\n Ber(p=0.25)")
lines(x=c(0,0,1,1),y=c(0,0.75,0,0.25), type = "h", lty = 2,col="blue")
curve(pbinom(x,size=1,prob=0.25),
      xlim=c(-1,2),col="blue",
      main="Función de distribución\n Ber(p=0.25)")
par(mfrow=c(1,1))
```

## Gráficas de la  distribución $Ge(p)$

```{r echo = FALSE}

#selectInput("n_breaks", label = "Number of bins:",
#             choices = c(10, 20, 35, 50), selected = 20)

sliderInput("p", label = "Probabilidad éxito p:",
              min = 0.01, max = 0.99, value = 0.25, step = 0.01)

renderPlot({
par(mfrow=c(1,2))
  p=input$p
plot(x=c(0,1),y=dbinom(c(0,1),size=1,prob=p),
     ylim=c(0,1),xlim=c(-0.5,2),xlab="x",pch=21,
     main=paste0(c("Función de probabilidad\n
                   Ber(p=",p,")"),collapse=""),bg="black")
segments(x0=0,y0=0,x1=0,y1=1-p, col = "blue", lty =2)
segments(x0=1,y0=0,x1=1,y1=p, col = "blue", lty =2)
segments(x0=-1,y0=1-p,x1=0,y1=1-p, col = "blue", lty =2)
segments(x0=-1,y0=p,x1=1,y1=p, col = "blue", lty =2)
x=0:1
y=pbinom(x,size=1,prob=p)
curve(pbinom(x,size=1,prob=p),
      xlim=c(-1,2),col="blue",
      main="Función de distribución\n Ber(p=0.25)")

par(mfrow=c(1,1))
})
```

# Distribución binomial


## Distribución binomial


<l class=definition> Distribución binomial </l>

Si repetimos $n$ veces de forma independiente un experimento Bernoulli de parámetro $p$.

El espacio muestral $\Omega$ estará formado por cadenas de $E$'s y $F$'s de longitud $n$
Consideremos la v.a.

$$X(\overbrace{EFFF\ldots EEF}^{n})=\mbox{número de éxitos en la cadena}.$$

## Función de probabilidad de una  binomial

Entonces su **función de probabilidad** es 

$$
P_{X}(x)=\left\{
\begin{array}{ll}
{n\choose x}\cdot  p^x \cdot(1-p)^{n-x} &\mbox{ si } x=0,1,\ldots,n\\
0  & \mbox{ en otro caso}
\end{array}\right..
$$

## Función de distribución de binomial

Su **función de distribución** no tiene una fórmula cerrada. Hay que acumular la función de probabilidad:

$$
\begin{eqnarray*}
F_{X}(x)=P(X\leq x) & = & \sum_{i=0}^x P_X(i)\\
& = & 
\left\{
\begin{array}{ll}
0 & \mbox{ si } x\leq 0\\\displaystyle
\sum_{i=1}^k {n\choose i}\cdot  p^i \cdot (1-p)^{n-i} & \mbox{ si } 
\left\{
  \begin{array}{l} 
  k\leq x< k+1\\
  k=0,1,\ldots,n.
  \end{array}
\right.\\
1 & \mbox{ si } n\leq x
\end{array}
\right..
\end{eqnarray*}
$$




## Números binomiales con R

Los números binomiales nos calculan cuanto equipos de baloncesto ($k=5$ jugadores) se pueden hacer con 6 jugadores ($n=6$).

Es decir cuántas  manearas distintas hay para elegir (*choose*) 5 jugadores en un conjunto de 6 jugadores. Todo el mundo diría 
¡¡¡6!!!. Efectivamente con R es 

```{r}
choose(6,5)
```

## Números binomiales con R

Con 10 jugadores  el número de equipos de  5 distintos es bastante más grande

```{r}
choose(10,5)
```

Y por ejemplo con un equipo de fútbol profesional  que tiene en plantilla 22 jugadores  (quitamos lo porteros) se pueden formar ¡¡nada menos que!!

```{r}
choose(22,10)
```
equipos distintos

##  Distribución Binomial

En las anteriores circunstancias diremos que la v.a. sigue una **ley de probabilidad binomial** ... con parámetros $n$ y $p$ y lo denotaremos así 
    
$$X\equiv B(n,p).$$ 
    
Obviamente se tiene que una bernoulli es una binomial con $n=1$

$$B(1,p)=Ber(p).$$

<div class="exercise"> Ejercicio:

Calculad las funciones de distribución de una binomial $B(n=1,p=0.3)$ y comprobar que coinciden con  las distribuciones de una $Ber(p=0.3)$.
</div>

## Observaciones sobre la distribución binomial

* La probabilidad de fracaso se suele denotar con  $q=1-p$, **sin ningún aviso adicional** con el fin de acortar la escritura de las  fórmulas.
* Su **función de distribución no tienen una formula general**, hay que calcularla con una función de R o python... En el siglo pasado se tabulaban en los libros de papel :-).
* En el material adicional os pondremos unas tablas de esta distribución
para distintos valores de $n$ y $p$ para que disfrutéis de tan ancestral método de cálculo. 
* Cualquier paquete estadístico, hoja de cálculo dispone de
funciones para el cálculo de estas probabilidades, así que el **uso de las tablas** queda **totalmente anticuado**. 

## Esperanza  de una $X$ con distribución $B(n,p).$

Su *esperanza* es 
$$E(X)=\displaystyle\sum_{k=0}^n k \cdot  {n \choose k }\cdot p^k\cdot q^{n-k} = n\cdot p.$$

La esperanza de $X^2$ es 

$$
\begin{eqnarray*}
E(X^2)&=& \displaystyle\sum_{k=0}^n k^2 \cdot  {n \choose k }\cdot p^k\cdot q^{n-k}\\
&=& n\cdot p\cdot q-(n\cdot p)^2.
\end{eqnarray*}
$$

## Varianza distribución $B(n,p)$.

Su **varianza** es 

 $$Var(X)=E(X^2)-\left(E(X)\right)^2=n\cdot p \cdot q=n\cdot p\cdot (1-p).$$

Su desviación  típica es
 
 $$\sqrt{n\cdot p\cdot q}=\sqrt{n\cdot p\cdot (1-p)}.$$





## Resumen v.a con distribución binomial $B(n,p)$

 $X$ binomial | $B(n,p)$ 
-------------:|:--------
$D_X=$ |  $\{0,1,\ldots n\}$ 
$P_X(x)=P(X=x)=$ |$\left\{\begin{array}{ll}{n\choose x}\cdot  p^x\cdot  (1-p)^{n-x} & \mbox{ si } x=0,1,\ldots,n\\0  & \mbox{ en otro caso.}\end{array}\right.$
$F_X(x)=P(X\leq X)=$ | no tiene fórmula (utilizad funciones de R o python)
$E(X)=$ |  $n\cdot p$
$Var(X)=$ | $n\cdot p \cdot (1-p)$

## Cálculos con R

Veamos los cálculos básicos con funciones de R para una v.a $X$ con distribución  binomial  $B(n=10,p=0.25)$. 

**Función de distribución** $P(X\leq 0)$ y $P(X\leq 4)$:

```{r binomialfuncionesA}
pbinom(0,size=10,prob=0.25)
pbinom(4,size=10,prob=0.25)
```


## Cálculos binomial con R

**Función de probabilidad** $P(X=0)$ y $P(X=4)$:

```{r binomialfunciones2A_dbinom}
dbinom(0,size=10,prob=0.25)
dbinom(4,size=10,prob=0.25)
```



## Cálculos binomial con R

**Generación de muestras aleatorias  binomial**

Generaremos una muestra aleatoria  de  100 valores  de una población $B(20,0.5)$

```{r semilla_binomial, include=FALSE}
set.seed(8102019)
```
```{r binomialfunciones2A_random}
rbinom(100,size = 20,prob=0.5)
```

<div class="example"> Ejemplo:
EL ejemplo anterior correspondería a repetir 100 veces el experimento lanzar una moneda  20 veces y contar el número de caras.
</div>

## Cálculos binomial con python

Veamos los cálculos básicos con funciones de python para una v.a $X$ con distribución  binomial  $B(n=10,p=0.25)$.

Primero importamos la  función `binom` de la librería `scipy.stat`

```{python binomialfunciones2A_python}
from scipy.stats import binom
```


## Cálculos binomial con python

**Función de distribución** $P(X\leq 0)$ y $P(X\leq 4)$

```{python bino3_py}
binom.cdf(0,n=10,p=0.25)
binom.cdf(4,n=10,p=0.25)
```

##  Cálculos binomial con python

**Función de probabilidad** $P(X=0)$ y $P(X=4)$:

```{python bino4_py_2}
binom.pmf(0,n=10,p=0.25)
binom.pmf(4,n=10,p=0.25)
```


##  Cálculos distribución binomial con python

**Generación de muestras aleatorias  binomial**. Generaremos una muestra aleatoria  de  100 valores  de una población $B(20,0.5)$

```{python bino4_py}
binom.rvs(n=100,p=0.25,size = 20)
```
</div class="observ"> Notemos que la secuencia aleatoria generada no es la misma que con R. De hecho volvemos a ejecutarla obtendremos una muestra aleatoria distinta.
</div>



## Cálculos binomial con python

Veamos los cálculos básicos con funciones de python para la binomial  $B(n=10,p=0.25)$.

Función de distribución

```{python binomialfunciones2A_python2}
binom.pmf(0,5,0.4)
binom.pmf(1,10,0.25)
binom.rvs(n=20,p=0.25,size=10)
```


## Gráficos de la distribución binomial con R

El siguiente código de R dibuja las función de probabilidad y la de distribución de una  $B(n=10,p=0.25)$


```{r fig.align='center',echo=FALSE}
par(mfrow=c(1,2))
aux=rep(0,22)
aux[seq(2,22,2)]=dbinom(c(0:10),size=10,prob=0.25)
plot(x=c(0:10),y=dbinom(c(0:10),size=10,prob=0.25),
  ylim=c(0,1),xlim=c(-1,11),xlab="x",
  main="Función de probabilidad\n B(n=10,p=0.25)")
lines(x=rep(0:10,each=2),y=aux, type = "h", lty = 2,col="blue")
curve(pbinom(x,size=10,prob=0.25),
  xlim=c(-1,11),col="blue",
  main="Función de distribución\n B(n=10,p=0.25)")
par(mfrow=c(1,1))
```

## Gráficas interactivas


```{r echo = FALSE}

#selectInput("n_breaks", label = "Number of bins:",
#             choices = c(10, 20, 35, 50), selected = 20)
# textInputRow<-function (inputId, label, value = 10, min=1,max=50,step=1 ) 
# {
#     div(style="display:inline-block",
#         tags$label(label, `for` = inputId), 
#         tags$input(id = inputId, type = "text", value = value,min=min,max=max,step=step,class="sliderInput"))
# }
#         textInputRow(inputId="n", label="Número de repeticiones n:", value = 10, min=1,max=50,step=1)
#         
#         textInputRow(inputId="p", label="x-max", value = 0.5)

fluidPage(
fluidRow(
  column(6, 
       sliderInput("n", label = "Número de repeticiones n:",
              min = 1, max = 50, value =10 , step = 1)
       ), 
       column(6, 
         sliderInput("prob", label = "Probabilidad éxito p:",
              min = 0.01, max = 0.99, value = 0.25, step = 0.01)
         )
  )
)

renderPlot({
  par(mfrow=c(1,2))
  n=input$n
  p=input$prob
  aux=rep(0,(n+1)*2)
  aux[seq(2,(n+1)*2,2)]=dbinom(c(0:n),size=n,prob=p)
  plot(x=c(0:n),y=dbinom(c(0:n),size=n,prob=p),
       ylim=c(0,1),xlim=c(-1,n+1),xlab="x",
       main=paste0(c("Función de probabilidad\n B(n=",n,",p=",p,")"),collapse = ""))
  lines(x=rep(0:n,each=2),y=aux, type = "h", lty = 2,col="blue")
  curve(pbinom(x,size=n,p=p),
        xlim=c(-1,input$n+1),col="blue",
        main=paste0(c("Función de distribución\n B(n=",input$n,",p=",p,")"),
                    collapse = ""))
        par(mfrow=c(1,1))
})

```

## Gráficos de la distribución binomial con python

<div class="exercise">
Ejercicio
Buscad en los manuales de python cómo se dibuja la función de probabilidad y de distribución de una binomial.
</div>

<div class="exercise-sol">
Necesitamos más librerías

```{python}
import numpy as np
import matplotlib.pyplot as plt
```


```{python dibu_python1,eval=FALSE}
n, p = 5, 0.4
x = np.arange(binom.ppf(0.01, n, p),binom.ppf(0.99, n, p))
plt.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')
plt.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)
```
</div>



## Gráficos de la distribución binomial con python (GGGGRANDES)
<div class="center"> 
```{python dibu_python2,echo=FALSE,fig.width=1,fig.height=1}
n, p = 5, 0.4
x = np.arange(binom.ppf(0.01, n, p),binom.ppf(0.99, n, p))
plt.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')
plt.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)
```
</div>



## Ejemplo distribución binomial

<div class="example">
**Ejemplo: Número de bolas rojas extraídas  de urna con reposición.**


Tenemos una urna con $100$ bolas de las cuales 40 son rojas  y 60  blancas. Extraemos al azar una bola, anotamos su color y  la devolvemos (la reponemos en la urna)  a la urna.

Supongamos que repetimos este proceso $n=10$; reponiendo en cada ocasión la bola extraída. 

Consideremos la variable aleatoria $X=$ número de bolas rojas extraídas (con reposición)  en $n=10$ repeticiones del miesmo experimento Bernoulli.


Bajo estas condiciones tenemos que  repetimos $n=10$ veces el mismo experimento Bernouilli con probabilidad de éxito (sacar bola roja) es 

$$P(Roja)=P(Éxito)=p=\frac{40}{100}=0.4.$$

Así que la variable $X=" número de bolas rojas extraídas de la urna en $n=10$ ocasiones sigue una ley binomial  $B(=n=10,p=0.4).$


## Ejemplo $B(n=10,p=0.4).$

Nos preguntamos:

1. ¿Cuál es la probabilidad de que saquemos exactamente $4$ rojas?
2. ¿Cuál es la probabilidad de que saquemos al menos  $4$ rojas?
3. ¿Cuál es la probabilidad de que saquemos  menos  $3$ rojas?
4. ¿Cuál es el valor esperado del número de bolas rojas?
5. ¿Cuál es la desviación típica  del número de bolas rojas?

</div>

## Ejemplo $B(n=10,p=0.4).$

<div class="exmaple-sol">
 
**Solución 1**. ¿Cuál es la probabilidad de que saquemos exactamente $4$ rojas?

Utilizando al función de probabilidad  tenemos que 

$$
\begin{eqnarray*}
P_x(x=4)&=&{10\choose 4}\cdot 0.4^4\cdot (1-0.4)^{10-4}
= \frac{10!}{(10-4)!\cdot 4!}\cdot 0.4^4\cdot 0.6^6\\
&=& \frac{7\cdot 8\cdot 9\cdot 10}{1\cdot 2\cdot 3\cdot 4}\cdot 0.4^4\cdot 0.6^6=`r round(choose(10,4)* 0.4^4*0.6^6,7)`
\end{eqnarray*}
.
$$


Con R

```{r}
dbinom(4,size=10,prob = 0.4)
```
## Ejemplo $B(n=10,p=0.4).$


**Solución 2**.  ¿Cuál es la probabilidad de que saquemos al menos  $4$ rojas?

Al menos 4 rojas es $P(X \geq  4)=1-P(X<4)=1-P(x\leq 3).$

Así que calculemos $P(X\leq3)$

$$
\begin{eqnarray*}
P(x\leq 3)&=& P(X=0)+P(X=1)+P(X=2)+P(X=3)\\
&=& 
 {10\choose 0}\cdot 0.4^0\cdot (1-0.4)^{10-0}+ {10\choose 1}\cdot 0.4^1\cdot (1-0.4)^{10-1}\\
&+&{10\choose 2}\cdot 0.4^2\cdot (1-0.4)^{10-2}+ {10\choose 3}\cdot 0.4^3\cdot (1-0.4)^{10-3}\\
&=&`r round(pbinom(3,10,0.4),7)`
\end{eqnarray*}
.
$$




## Ejemplo $B(n=10,p=0.4).$

Con R

```{r}
pbinom(3,10,0.4)
```


Así que

$$P(X \geq 4 )=1-P(x< 4)=P(X\leq 3)=1-`r round(pbinom(3,10,0.4),7)`=`r round(1-pbinom(3,10,0.4),7)`.$$


## Ejemplo $B(n=10,p=0.4).$

O con R también

```{r}
1-pbinom(3,10,0.4)
pbinom(3,10,0.4,lower.tail = FALSE)
```


## Ejemplo $B(n=10,p=0.4).$

**Solución 3**.  ¿Cuál es la probabilidad de que saquemos  menos  $3$ rojas?

$$
\begin{eqnarray*}
P(x< 3)&=& P(X\leq 2)=  P(X=0)+P(X=1)+P(X=2)\\
&=& 
 {10\choose 0}\cdot 0.4^0\cdot (1-0.4)^{10-0}+ {10\choose 1}\cdot 0.4^1\cdot (1-0.4)^{10-1}\\
&+&{10\choose 2}\cdot 0.4^2\cdot (1-0.4)^{10-2}\\
&=&`r round(pbinom(2,10,0.4),7)`
\end{eqnarray*}
.
$$



## Ejemplo $B(n=10,p=0.4).$

**Solución 4**. ¿Cuál es el valor esperado del número de bolas rojas?

Como  $X$ es una $B(n=10,p=0.4)$ sabemos que 

$$E(X)=n\cdot p = 10\cdot 0.4=`r 10*0.4`.$$

## Ejemplo $B(n=10,p=0.4).$

**Solución 5**. ¿Cuál es la desviación típica  del número de bolas rojas?


La varianza es 
$$Var(X)=n\cdot p \cdot(1-p)=10\cdot 0.4\cdot 0.6=`r 10*0.4*0.6`.$$


Pro lo tanto la desviación típica es 

$\sqrt{Var(X)}=\sqrt{`r 10*0.4*0.6`}= `r sqrt(10*0.4*0.6)`.$
</div>


# Distribución geométrica


## Distribución geométrica


<l class="definition"> Distribución geométrica </l>

* Repitamos un experimento Bernoulli, de parámetro p, de forma independiente hasta obtener el primer éxito.
* Sea $X$ la v.a. que cuenta el número de fracasos antes del primer éxito. Por ejemplo  que  hayamos tenido  $x$ fracasos  será una cadena de $x$ fracasos culminada con un éxito. Más concretamente 

$$P(\overbrace{FFF\ldots F}^{x}E)=P(F)^{x}\cdot P(E)=(1-p)^{x}\cdot p=q^{x}\cdot p.$$


## Distribución Geométrica

Su función de probabilidad es 

$$
P_X(x)=P(X=x)=\left\{\begin{array}{ll}
(1-p)^{x}\cdot p & \mbox{ si } x=0,1,2,\ldots\\
0 &\mbox{ en otro caso}
\end{array}\right..
$$




* De una v.a. como esta diremos que sigue una
distribución geométrica de parámetro $p$..
* La  denotaremos por $Ge(p)$. 
* Su dominio es  $D_X=\{0,1,2,\ldots\}$.


## Función de distribución geométrica

Calculemos P($X\leq 3$).

Por la propiedad de la probabilidad del suceso complementario tenemos que

$$
P(X\leq 3 )=1-P(X> 3)=1-P(X\geq 4)
$$

 Efectivamente, el  evento  tenemos que $X\leq 3$  es que hemos fracasado más de tres veces hasta conseguir el primer éxito; es decir **hemos fracasado 4 o más veces**, por lo tanto 


$$
\{X>3\}=\{X\geq 4\}= \{FFFF\}
$$

## Función de distribución geométrica

Ahora, al  ser los intentos sucesos independientes, tenemos que:

$$
\begin{eqnarray*}
P(X>3) & = & P(\{FFFF\})= P(F)\cdot P(F)\cdot P(F)\cdot P(F)\\
&=& (1-p)\cdot (1-p)\cdot (1-p)\cdot (1-p)= (1-p)^{3+1}\\\
&=&(1-p)^{4}.
\end{eqnarray*}
$$


Ahora calculamos 

$$F_X(3)=P(X\leq 3)=1-P(X>3)=1-(1-p)^{3+1}.$$

Ahora podemos generalizar a cualquier entero positivo $k=0,1,2,\ldots$  

$$F_X(k)=P(X\leq k)=1-(1-p)^{k+1}\mbox{ si } k=0,1,2,\ldots$$

## Función de distribución geométrica


En general  tendremos que 


$$
F_X(x)=P(X\leq x)=
\left\{\begin{array}{ll} 
0 & \mbox{ si } x<0\\
1- (1-p)  & \mbox{ si } k=0\leq x <1\\
1- (1-p)^2 & \mbox{ si } k=1\leq x <2\\
1- (1-p)^3 & \mbox{ si } k=2\leq x <3\\
1- (1-p)^{k+1} & \mbox{ si } \left\{ \begin{array}{l}k\leq x< k+1\\\mbox{para } k=0,1,2,\ldots\end{array}
    \right.\end{array}\right.
$$

## Propiedad de la falta de memoria

<l class="prop"> Propiedad de la falta  de memoria </l>

Sea $X$ una v.a. discreta con dominio $D_X=\{0,1,2,\ldots\}$  y con $P(X=0)=p$.

Entonces $X$ sigue una ley $Ge(p)$ sí y sólo si

$$P\left(X> k+j\big| X\geq j\right)=P(X> k)$$
para todo $k,j=1,2,3\ldots$.


## Propiedad de la falta de memoria

<div class="dem">
**Demostración**
Si es geométrica entonces el lado derecho de la igualdad es 

$$
P(X > k)=1-P(X\leq k)=1-\left(1-(1-p)^{k+1}\right)=(1-p)^{k+1}
$$

El lado de izquierdo es

$$
\begin{eqnarray*}
P\left(X> k+j\big| X\geq j\right)&=&\frac{P\left(\{X> k+j\}\cap \{X\geq j\} \right)}{P\left(X\geq j\right)}=
\frac{P\left(X>k+j \right)}{P\left(X\geq j \right)} = \frac{1-P(X\leq k+j)}{1-P(X\leq j-1)}\\
&=&  \frac{1-(1-(1-p)^{k+j+1})}{1-(1-(1-p)^{j-1+1})} =\frac{(1-p)^{k+j+1}}{(1-p)^{j}} = (1-p)^{k+1}
\end{eqnarray*}
$$


Lo que demuestra  la igualdad.
</div>

## Propiedad de la falta de memoria

<div class="dem">
Ahora suponemos que cumple la propiedad de la  la falta de memoria 
$$
P(X > k)=1-P(X\leq k)=1-\left(1-(1-p)^{k+1}\right)=(1-p)^{k+1}
$$

El lado de izquierdo es

$$
\begin{eqnarray*}
P\left(X> k+j\big| X\geq j\right)&=&\frac{P\left(\{X> k+j\}\cap \{X\geq j\} \right)}{P\left(X\geq j\right)}=
\frac{P\left(X>k+j \right)}{P\left(X\geq j \right)} = \frac{1-P(X\leq k+j)}{1-P(X\leq j-1)}\\
&=&  \frac{1-(1-(1-p)^{k+j+1})}{1-(1-(1-p)^{j-1+1})} =\frac{(1-p)^{k+j+1}}{(1-p)^{j}} = (1-p)^{k+1}
\end{eqnarray*}
$$


Lo que demuestra  la igualdad.
</div>

## Falta de memoria

<l class="observ"> Observación: Interpretación  de la propiedad</l>

 La propiedad  de la falta de memoria
$$
P(X> k+j|X> j)=P(X\geq k)
$$   

la igualdad anterior significa  que aunque **ya llevemos al menos  $j$ fracasos** la probabilidad de **que fracasemos $k$ veces más** no disminuye; es la misma  que si empezáramos de nuevo el experimento. 

A este efecto se le suele etiquetar con la frase  *el experimento carece de memoria* o es un *experimento sin memoria*.
</div>

## Falta de memoria

Un ejemplo muy sencillo nos aclarará el alcance de esta propiedad

<div class="example"> **Ejemplo**: Llavero 
Tenemos un llavero con 10 llaves,  solo una de ellas abre una puerta. Cada vez que probamos una llave y falla olvidamos que llave hemos probado. ¿Cuál es la probabilidad de que  si ya lo hemos intentado  5 veces necesitemos más de 4 intentos adicionales  para abrir la puerta?
</div>

<div class="example-sol">

Tomemos $k=4,j=5$, aplicando la propiedad de la falta de memoria

$$
P(X> 4+5/X \geq 5)=P(X > 4)
$$

Después de 5 fracasos no estamos *más cerca* de abrir la puerta.
La propiedad de la  falta de  memoria  nos dice que en después de cada intento es como si empezásemos  de nuevo a abrir la puerta. Tras 5 fracasos la probabilidad de que fallemos más de  4 veces  más es la misma.
</div>

## La variable geométrica que cuenta los intentos para obtener el primer éxito.

* Supongamos que sólo estamos interesados en el número de intentos para obtener el
primer éxito. 
* Si definimos $Y$= número de  intentos para obtener el  primer éxito. Entonces $Y=X+1$  donde $X\equiv Ge(p)$.
* Su dominio es valores en $\{1,2,\ldots\}$ 
* $E(Y)=E(X+1)=E(X)+1=\frac{1-p}{p}+1=\frac1{p}$.
* $Var(Y)=Var(X+1)=Var(X)=\frac{1-p}{p^2}$.


## Variable geométrica: Madrid/Barça, Barça/Madrid

<div class="example"> 
**Ejemplo**:  Número de partidos de fútbol necesarios para ver ganar el Barça al Madrid:

Los partidos Real Madrid FC Barcelona de **la liga** española se suelen denominar  **El Clásico**, sean en el Bernabeu (estadio del Real Madrid) o en el Camp Nou (estadio del Barça)

Sea $Y$ la variable que cuenta el número de veces que  en un partido de fútbol de la liga el Real Madrid  pierde contra el Barça sea en el Camp Nou  o el Calderón.

Nuestra amiga Aina es muy culé (hincha del Barça) quiere averiguar cuántos partidos consecutivos de **EL Clásico** tiene que ver hasta ver ganar al Barça por primera vez. 

Le interesa estimar cuánto le va a costar este capricho. Tendrá que comprar las entradas y pagar los viajes de Barcelona a Madrid.

</div>

## Variable geométrica: Madrid/Barça, Barça/Madrid



<div class="example"> 

Le interesa estimar cuánto le va a costar este capricho. Tendrá que comprar las entradas y pagar los viajes de Barcelona a Madrid.

Para ello  consulta [datos historicos de **El clásico**  en la wikipedia](https://es.wikipedia.org/wiki/El_Cl%C3%A1sico)  y  averigua que hasta el 3 de marzo de 2019  el Real Madrid ganó en 72 ocasiones el Barça en 72  y empataron 34 veces, en total se  han jugado 178 
**Clásicos**.

La pregunta es:

¿Cuál es la probabilidad de que tenga que ver al menos tres partidos hasta que el Barça gane el cuarto?

</div>

## Madrid/Barça, Barça/Madrid

<div class="example-sol"> 

Como nuestro aficionada tiene un tía que juega mucho al fútbol en su consola  y con los datos que le damos  estima que  la probabilidad de que el Barça gane un clásico cualquiera es

$$P(Barça)=\frac{72}{178}=`r round(72/178,4)`.$$

Supongamos  pues que nuestro  *experimento de fútbol*  sigue una ley  geométrica con probabilidad de éxito $p=P(Barça)=\frac{72}{178}.$

Entonces si $X$ es el número de partidos que pierde el Barça antes de que gane el primero  seguirá  una ley $Ge(p=\frac{72}{178})$.

Así que  lo que nos pregunta Aina es la siguiente probabilidad

$$P(X>3)=1-P(X\leq 3)=1-\left(1-\frac{72}{178}\right)^4=`r round(1-(1-72/178)^4,4)`.$$


Así que  tienen una probabilidad del $87.42\%$ de que necesite ver al menos no ganar al Barça 3 partidos antes de ver uno en el que gane.
</div>

## Madrid/Barça, Barça/Madrid

<div class="example"> 
**Ejemplo**

Bajo las condiciones del problema Barça-Madrid de nuestra amiga Aina ¿cuál es el valor esperado de $X$? ¿y su varianza? 
 
</div>
<div class="example-sol"> 

$X=Ge(p=\frac{72}{178}=`r round(72/178,4)`)$ 

entonces 

$E(X)=\frac{1-p}{p}=\frac{1-`r round(72/178,4)`}{`r round(72/178,4)`}=`r round((1- (72/178))/(72/178),4)`$ 

y

$Var(X)=\frac{1-p}{p^2}=\frac{1-`r round(72/178,4)`}{(`r round(72/178,4)`)* 2}=`r round((1- (72/178))/(72/178)^2,4)`$

</div>


## Resumen $Ge(p)$ empezando en 0

$Y=$ | número de fracasos  para conseguir el primer éxito
|------:|:-----|
|*Geométrica* |  que empieza en $0$|
|$D_X=$ | $\{0,1,\ldots n\}$ | 
$P_X(x)=P(X=x)=$ |$\left\{\begin{array}{ll}(1-p)^{x}\cdot p & \mbox{ si } x=0,1,\ldots,n\\0  & \mbox{ en otro caso.}\end{array}\right.$
|$F_X(x)=P(X\leq X)=$ | $\left\{\begin{array}{ll} 0 & \mbox{ si } x<0\\
  1- (1-p)^{k+1} & \mbox{ si } \left\{ \begin{array}{l}k\leq x< k+1\\\mbox{para } k=1,2,\ldots\end{array}
    \right.\end{array}\right.$ |
|$E(X)=\frac{1-p}{p}$ | $Var(X)=\frac{q}{p^2}$|



## Resumen $Ge(p)$ comenzando en $1$.

$X=$ |número de INTENTOS  para OBTENER el primer éxito
------:|:-----
|*Geométrica*|$Ge(p)$, $q=1-p$.|
|$D_X=$ |  $\{1,\ldots n\}$ | 
|$P_X(x)=P(X=x)=$ |$\left\{\begin{array}{ll}(1-p)^{x-1}\cdot p & \mbox{ si } x=1,\ldots,n\\  0  & \mbox{ en otro caso.}\end{array}\right.$|
|$F_X(x)=P(X\leq X)=$ | $\left\{\begin{array}{ll} 0 & \mbox{ si } x<1\\ 1- q^{k} & \mbox{ si } \left\{ \begin{array}{l}k\leq x< k+1\\\mbox{para } k=1,2,\ldots\end{array}    \right.\end{array}\right.$ |
|$E(X)=\frac1{p}$ |$Var(X)=\frac{1-p}{p^2}$|


## Cálculos con R

Veamos los cálculos básicos con  R para la distribución geométrica  $Ge(p=0.25)$ que cuenta el número de fracasos antes del primer éxito es decir empezando en $0$

```{r binomialfuncionesA011111}
dgeom(0,prob=0.25)
pgeom(0,prob=0.25)
dgeom(1,prob=0.25)
```


## Cálculos con R

Veamos los cálculos básicos con  R para la distribución geométrica  $Ge(p=0.25)$ empezando en $0$

```{r binomialfuncionesA0}
pgeom(1,prob=0.25)
rgeom(n=20,prob=0.25)
```



## Gráficos $Ge(p=0.25)$.

El siguiente código dibuja las función de probabilidad y la de distribución de una  $Ge(p=0.25)$

```{r eval=FALSE}
par(mfrow=c(1,2))
plot(x=c(0:10),y=dgeom(c(0:10),prob=0.25),
  ylim=c(0,1),xlim=c(-1,11),xlab="x",
  main="Función de probabilidad\n Ge(p=0.25)")
lines(x=rep(0:10,each=2),y=aux, type = "h", lty = 2,col="blue")

aux0=dgeom(c(0:10),prob=0.25)
ceros=rep(0,21)
ceros
aux=ceros
aux[2*(c(1:11))]<-aux0
curve(pbinom(x,size=10,prob=0.25),
curve(pgeom(x,prob=0.25),
  xlim=c(-1,10),col="blue",
  main="Función de distribución\n Ge(p=0.25)")
par(mfrow=c(1,1))
```


##  Los gráficos


```{r graficos22, fig.align='center',echo=FALSE}
par(mfrow=c(1,2))
plot(x=c(0:10),y=dgeom(c(0:10),prob=0.25),
  ylim=c(0,1),xlim=c(-1,11),xlab="x",
  main="Función de probabilidad\n Ge(p=0.25)")
curve(pgeom(x,prob=0.25),
  xlim=c(-1,10),col="blue",
  main="Función de distribución\n Ge(p=0.25)")
par(mfrow=c(1,1))
```

# Distribución binomial negativa

## El problema de la  puerta con dos cerraduras

Repitamos el problema de abrir la puerta. Tenemos un llavero con 10 llaves distintas y tenemos que abrir una puerta con **dos cerraduras**  empezamos por la primera cerradura  y cada vez olvidamos que llave hemos probado, una vez abierta la primera cerradura probamos con la segunda hasta que también la abrimos.

Sea $X=$ número de intentos necesarios hasta abrir la  puerta.

# Distribución binomial negativa

La idea de la puerta es que tenemos un experimento Bernoulli con probabilidad de éxito $p=0.1$. Y repetiremos el experimento hasta obtener 2 éxitos.

En general tendremos un experimento  Bernoulli con probabilidad de éxito $0<p<1$ y  tal que:

* Repetimos el experimento hasta obtener el $r$-ésimo éxito (abrir la maldita puerta). 
* Sea $X$ la v.a que cuenta el número de repeticiones del experimento hasta el r-ésimo
éxito. 
* Su función de probabilidad es

$$
P_{X}(x)=P(X=x)=\left\{\begin{array}{ll}
     {{x-1}\choose{r-1}} (1-p)^{x-r}p^r & \mbox{si } x=r,r+1,\ldots\\
     0 & \mbox{en otro caso}\end{array}\right.
$$
     

## Distribución binomial negativa

* Una v.a. con este tipo de distribución recibe el nombre de *binomial negativa* y la denotaremos por $BN(p,r)$. Notemos que $BN(p,1)=Ge(p)$.

Nota: Dados dos enteros positivos $n$ y $k$ se  define en numero binomial negativo como 

$$\binom{-n}{k}=\frac{(-n)(-n-1)\cdots (-n-k+1)}{k!}.$$




Se cumple que  

$$
(t+1)^{-n}=\sum_{k=0}^{+\infty}\left(\begin{array}{c} -n
\\ r\end{array}\right) t^{k}
$$

<!-- 


## Distribución binomial negativa NO se incluye son propiedades de los números binomiales negativos

Además 

$$
\left(\begin{array}{c} x-1
\\ r-1\end{array}\right)\left(\begin{array}{c} -n
\\ r\end{array}\right)
$$

-->


## Resumen Binomial Negativa $BN(r,p)$

$X=$ | número de intentos para conseguir el $r$-ésimo éxito
*Binomial negativa* $BN(r,p)$ | $r$ éxitos, probabilidad de éxito $p$, $q=1-p$
--------------:|:------------
$D_X=$ | $\{r,1,2,3\ldots\}$  
$P_X(x)=P(X=x)=$ | $\left\{
\begin{array}{cc}
\left(
\begin{array}{c}
x-1\\ r-1
\end{array}
\right)\cdot
q^{x-r}\cdot p^r & \mbox{si }  x=r,r+1,\ldots \\
0 & \mbox{en otro caso.}
\end{array}
\right.$
|$F_X(x)=P(X\leq X)=$ | no tiene fórmula (utilizar funciones de R o python.)
$E(X)=\frac{r}{p}$|$Var(X)=\frac{r\cdot q}{p^2}$ 

# Distribución Poisson

## Distribución Poisson

* Diremos que una v.a. discreta $X$ con $X(\Omega)=\mathbf{N}$ tiene distribución de Poisson con parámetro $\lambda>0$, y lo denotaremos
por $Po(\lambda)$ si su función de probabilidad es:

$$P_{X}(x)=P(X=x)=
\left\{\begin{array}{ll}
\frac{\lambda^x}{x!} e^{-\lambda}& \mbox{ si } x=0,1,\ldots\\
0 & \mbox{en otro caso}\end{array}\right..$$



## Distribución Poisson

* Recordemos que el desarrollo en serie  Taylor de la exponencial es $$e^{\lambda}=\sum_{x=0}^{+\infty} \frac{\lambda^x}{x!}.$$

Teniendo en cuenta esto es  fácil comprobar que  todos los valores de la función de probabilidad suman 1.

## La distribución Poisson como "límite" de una binomial.

* La distribución Poisson aparece en el conteo de determinados  eventos que se
producen en un intervalo de tiempo o en el espacio.
* Supongamos que nuestra variable de interés es  $X$= número de eventos en el intervalo de tiempo $(0,t]$ 
* por ejemplo el número de llamadas a un *call center* y que sabemos que se cumplen las siguientes condiciones:

## La distribución Poisson como "límite" de una binomial.




1. El número promedio de eventos en el intervalo $(0,t]$ es
$\lambda>0$.
2. Es posible dividir el intervalo de tiempo en un
gran número de subintervalos (denotemos por $n$ al número de intervalos) de forma que:
    + La probabilidad de que se produzcan dos o más eventos en un subintervalo es despreciable.
    + El número de ocurrencias de eventos en un intervalo  es independiente del número de ocurrencias en otro intervalo.
    + La probabilidad de que un evento ocurra en un subintervalo es $p=\frac{\lambda}{n}$·

## La distribución Poisson como "límite" de una binomial.

* Bajo estas condiciones podemos considerar que el número de eventos en el intervalo $(0,t]$ será el número de "éxitos" en $n$ repeticiones independientes de un proceso Bernoulli de parámetro $p$
*  Entonces si $n\to\infty$ y $p\cdot n$ se mantiene igual a $\lambda$ resulta que la función de probabilidad de $X$ se puede poner como

$$f_{X}(k)=\lim_{n\to\infty}\left(\begin{array}{c} n\\ k\end{array}\right)
p^k q^{n-k}= \frac{\lambda^k}{k!} e^{-\lambda}
$$


## Procesos de Poisson


<l class="prop"> Procesos de Poisson </l>

Consideremos  un experimento *Poisson*  con $\lambda$ igual
al promedio de eventos en una unidad de tiempo (u.t.).

Si $t$ es una cantidad de tiempo en u.t., la v.a.
$X_{t}$=numero de eventos en el intervalo $(0,t]$
es una $Po(\lambda\cdot t)$.

El conjunto de variables $\{X_t\}_{t>0}$ recibe el nombre de **proceso de Poisson**.



## Resumen distribución  Poisson  $Po(\lambda)$

$X$ Poisson |  $\lambda$.}
-------|-------
$D_X=$|  $\{0,1,\ldots n\}$ 
$P_X(x)=P(X=x)=$ | $\left\{\begin{array}{ll}  \frac{\lambda^x}{x!}\exp{-\lambda} & \mbox{ si } x=0,1,\ldots,n\\ 0  & \mbox{ en otro caso.}\end{array}\right.$
$F_X(x)=P(X\leq X)=$ |  Función de R o tabulada  
$E(X)=\lambda$ | $Var(X)=\lambda$

## Aproximación de la distribución binomial por la Poisson

Bajo el punto de vista anterior y si $p$ es pequeño y $n$
suficientemente grande (existen distintos criterios por ejemplo $n>20$ ó $30$ y  $p\leq 0.1$)
podemos aproximar una $B(n,p)$ por una $Po(n\cdot p)$

# Distribución hipergeométrica


## Modelo de la distribución hipergeométrica

Es la que modeliza matemáticamente, por ejemplo, el número de bolas blancas extraídas de una urna sin reposición.

Sean $n$,$m$ y $k$tres número esteros positivos y tales  que $k<m+n$.

Sea una urna que contiene $m+n$ bolas de las que
$m$ son blancas y las restantes $n$ no.

El número total de bolas es $m+n$. Extraemos $k$ bolas de la urna sin reemplazarlas.

## Modelo de la distribución hipergeométrica


Sea $X$ la v.a. que cuenta el número de bolas blancas extraídas.
Entonces

$$
P_{X}(x)=\left\{
\begin{array}{ll}
\frac{\binom{m}{x}\cdot \binom{n}{k-x}}{\binom{m+n}{k}} & \mbox{ si }
\max\{0,k-n\}\leq x \leq \min\{m,k\} \mbox { para  } x\in \mathbf{N}\\
0  & \mbox{en otro caso}\end{array}\right.
$$


## Distribución hipergeométrica

Una v.a. hipergeométrica con los  parámetros anteriores la
denotaremos por $H(m,n,k)$.

<div class="obsv"> 
En ocasiones se parametriza una v.a. hipergeométrica mediante $N=m+n$ número total de bolas, 
$k$=número de extracciones y $p=$ probabilidad de una bola blanca. Así podemos poner $H(N,n,p)$ donde $p=\frac{m}{N}$.
</div>



## Resumen  hipergeométrica  $H(m,n,k)$.

$X=$ | número de bolas blancas  en $k$ extracciones  sin reposición de una urna con $m$ bolas blancas y $n$ negras.

$D_X$=| $\begin{array}{l}D_X=\\ \{x\in\mathbf{N}\mid  \max\{0,n-N_2\}\leq x\\ x \leq \min\{n,N_1\}\}\end{array}$

$P_X(x)=P(X=x)=$ | $\begin{array}{l}F_X(x)=\\ P(X\leq X)=\end{array}$ 

$E(X)= $  | $Var(X)$ 
$\left\{\begin{array}{ll}
     \frac{{{N_1}\choose{x}}{{N_2}\choose{n-x}}}{{{N}\choose{n}}} & \mbox{ si }
   x\in D_X
      \\ 0  & \mbox{en otro caso}\end{array}\right.$|\begin{tabular}{c}No tiene\\ expresión.\end{tabular} |
$\frac{n N_1}{N}$ | $n\frac{N_1}{N}\left(1-\frac{N_1}{N}\right) \frac{N-n}{N-1}$




# Distribuciones Continuas. 

## Introducción. 
   
Al igual que en el caso discretos veremos distintos tipos de v.a. continuas que son
utilizadas de forma muy frecuente.

Una v.a. continua $X$ diremos que tiene una distribución uniforme sobre el intervalo real
$(a,b)$ ,$(a<b)$, si su función de densidad es 
$$f_X(x)=\left\{\begin{array}{ll}
\frac1{b-a} & \mbox{si } a<x<b\\ 0  & \mbox{en cualquier otro caso}
\end{array}
\right. $$ 

Como ejercicio comprobar que el área comprendida entre $f_X$ y la horizontal
vale 1.

## Función de distribución uniforme.

Entonces su función de distribución es

$$F_X(x)=\left\{\begin{array}{ll} 0  & \mbox{si } x\leq a\\
\frac{x-a}{b-a} & \mbox{si } a<x<b\\ 1  & \mbox{si } b\leq x
\end{array}
\right. $$


## Función de distribución uniforme:  cálculo.

Efectivamente:

* Si $x\leq a$ entonces $F_X(x)=\displaystyle\int_{-\infty}^{x} f(t)\cdot dt= \displaystyle\int_{-\infty}^{x} 0\cdot dt= \left[1\right]_{-\infty}^{x}=1-1=0$
* Si $a<x<b$ entonces 

\begin{eqnarray*}
F_X(x)&=&\int_{-\infty}^{x} f(t)\cdot dt= \int_{-\infty}^{a} 0\cdot dt+\int_{-\infty}^{x} \frac1{b-a} \cdot dt=\\
& & 1\mid_{-\infty}^{x}+\frac{t}{b-a}\mid_{a}^{x}=(1-1) +\frac{x}{b-a}-\frac{t}{b-a}=\frac{x-a}{b-a}.
\end{eqnarray*}


* Por último si $x\geq b$ entonces $F_X(x)\int_{-\infty}^{x} f(t) dt=1$ (ejercicio).


Si $X$ es una v.a. uniforme en el intervalo $(a,b)$ escribiremos $X\equiv U(a,b)$.


## Esperanza y varianza  para una v.a. $X\equiv U(a,b)$}

$E(X)=\int_{-\infty}^{+\infty} x f_X(x) dx=\int_{-\infty}^{+\infty} x \frac1{b-a} dx =
\frac{x^2}{2(b-a)}\mid _{a}^{b}=\frac{b+a}2$

$E(X^2)=\int_{-\infty}^{+\infty} x^2 f_X(x) dx=\int_{-\infty}^{+\infty} x^2 \frac1{b-a}
dx =\frac{x^3}{3(b-a)}\mid_{a}^{b} =\frac{b^3-a^3}{3(b-a)}=\frac{b^2+ab+a^2}3$

$Var(X)=E(X^2)-(E(X))^2=\frac{b^2+ab+a^2}3-(\frac{b+a}2)^2=\frac{(b-a)^2}{12}$


## Gráficas de la densidad y  distribución de una uniforme




\begin{figure}[h]
\begin{center}
\begin{tabular}{cc}       \includegraphics[scale=0.75]{densidaduniforme12}
&

       \includegraphics[scale=0.75]{distribucionuniforme12}\\ a) & b) \end{tabular}
\end{center}
       \caption{ Gráficas de la función de densidad (a)  y de la función de distribución (b) de una v.a. $U(-1,2)$.}
        \end{figure}%%%%%%%%  Sea $X$ una v.a. continua diremos que tiene distribución
%%%%%%%%          uniforme en el intervalo $(a,b)$ si su función de distribución
%%%%%%%%          es
%%%%%%%%          $$F_{X}(x)=\left\{\begin{array}{ll} 0 & \mbox{ si } x\leq a\\
%%%%%%%%          \frac{x-a}{b-a} & \mbox{ si } a\leq x\leq\\
%%%%%%%%          1 & \mbox{ si } b\leq x\end{array}\right.$$
%%%%%%%%
%%%%%%%%          $F$ es absolutamente continua y tiene por densidad:
%%%%%%%%         $$f_{X}(x)=\left\{\begin{array}{ll} \frac1{b-a} & \mbox{ si }
%%%%%%%%         a<x<b\\
%%%%%%%%         0 & \mbox{en el resto de casos}\end{array}\right.$$






## Cambio lineal v.a. uniforme.


Si $X$ sigue una distribución $U(a,b)$ entonces  $Z=\frac{x-a}{b-a}$ sigue una distribución $U(0,1)$.

En general si $d$ y $e$ son dos constantes reales  $T=d\cdot X+e$ sigue una ley $U(d\cdot a +e,d\cdot b +e)$  si $d>0$, cuando $d$ sea negativo $T$ sigue una ley 
$U(d\cdot b +e,d\cdot a +e)$. Las demostración se dejan como ejercicios.






## Resumen v.a con distribución uniforme, $U(a,b)$

Distribución uniforme | $U(a,b)$
----:|:-----
Dominio | $D_X=(a,b)$
$f_{X}(x)$ |$\left\{\begin{array}{ll}\frac1{b-a} & \mbox{si } a<x<b\\ 0  & \mbox{en cualquier otro caso}\end{array} \right.$
$F_X(x)=P(X\leq X)=$ |  $\left\{\begin{array}{ll} 0 & \mbox{ si } x\leq a\\\frac{x-a}{b-a} & \mbox{ si } a\leq x\leq\\1 & \mbox{ si } b\leq x\end{array}\right.$
 $E(X)=$ |$\frac{a+b}2$
 $Var(X)=$| $\frac{(b-a)^2}{12}$



## SIGUEEEEE
Supongamos que tenemos un proceso Poisson con parámetro $\lambda$ en una unidad de tiempo.

         Sea $t$ una cantidad de tiempo en u.t. entonces $N_{t}=$ número de
         eventos en el intervalo de tiempo $(0,t]$
         es una $Po(\lambda\cdot t)$. Consideremos la v.a.
         $T=$tiempo transcurrido entre dos eventos Poisson consecutivos.

         Sea $t>0$, entonces
         $$P(T>t)=P(\mbox{Cero eventos en el
         intervalo}(0,t])
         =P(N_{t}=0)=
         \frac{(\lambda t)^0}{0!} e^{-\lambda
         t}=e^{-\lambda t}.$$





         Tomando complementarios, la función de distribución de $T$ será

         $$F_{T}(t)=P(T\leq t)=\left\{\begin{array}{ll} 0 &\mbox{ si } t\leq 0\\
          1-P(T>t)=1-e^{-\lambda t}& \mbox{ si } t>0\end{array}\right.$$

         Entonces

         $$f_{T}(t)=\left\{\begin{array}{ll}
         \lambda e^{-\lambda t} & \mbox{ si }  t>0\\
         0 & \mbox{ si } t\leq 0
         \end{array}\right.$$

         La exponencial se denota por $Exp(\lambda)$




\subsubsection{Propiedad de la falta de memoria}

          Sea $X$  una v.a. $Exp(\lambda)$ entonces

          $$P(X>s+t/X>s)=P(X>t)\mbox{  para todo } s,t\in \mathbb{R}$$

          Toda v.a. absolutamente continua, que tome valores positivos
          y que verifique la propiedad de la falta de memoria es una v.a.
          exponencial.






## Resumen v.a con distribución exponencial, $Exp(\lambda)$

Sea $X\equiv Exp(\lambda).$

\scriptsize
\begin{tabular}{|c|c|c|c|c|}
\hline \begin{tabular}{c} Valores\\ admisibles.\end{tabular} & $f_{X}(x)$ & $F_X(x)=P(X\leq
X)=$ &
 $E(X)$ & $Var(X)$\\\hline & & & &\\
 $D_X=(0,+\infty)$ & $\left\{\begin{array}{ll}
         \lambda e^{-\lambda x} & \mbox{ si }  x>0\\
         0 & \mbox{ si } x\leq 0
         \end{array}\right.$ &  $F_{X}(x)=P(X\leq x)=\left\{\begin{array}{ll} 0 &\mbox{si } x\leq 0\\
          1-e^{-\lambda x}& \mbox{si } x>0\end{array}\right.
$ & $\frac1{\lambda}$ & $\frac1{\lambda^2}$ \\& & & &\\ \hline
\end{tabular}

\normalsize







          


          Diremos que una v.a. $X$ sigue una ley normal de parámetros
          $\mu$ y $\sigma^2$ y lo denotaremos por $N(\mu,\sigma^2)$
          si tiene por función de densidad

          $$f_{X}(x)=\frac1{\sqrt{2\pi}\sigma}
          e^{\frac{-(x-\mu)^2}{2\sigma^2}}\mbox{ para todo }x\in \mathbb{R}$$

          La gráfica de esta función es la conocida campana de Gauss.

          La v.a. normal con $\mu=0$ y $\sigma=1$ recibe el nombre de
          normal estándar y se suele denotar por la letra $Z$.


%%%%%%%%          \textbf{Propiedad} Sea $X$ una v.a. $N(\mu,\sigma^2)$ y sea
%%%%%%%%          $f_{X}$ su función de densidad. Entonces:
%%%%%%%%          \vskip -1cm
%%%%%%%%          \begin{enumerate}[a)]
%%%%%%%%          * Evidentemente $f_{X}$ verifica todas las pro\-pie\-da\-des de las
%%%%%%%%          funciones de densidad.
%%%%%%%%          * $f_{X}(\mu-x)=f_{X}(\mu+x)$ es simétrica respecto de la recta
%%%%%%%%          $x=\mu$
%%%%%%%%          * $f_{X}$ alcanza el máximo en $x=\mu$
%%%%%%%%         * Si $F_{X}$ la función de distribución de $X$ entonces
%%%%%%%%         $F_{X}(\mu+x)=1-F_{X}(\mu-x)$. En par\-ti\-cu\-lar si $Z$ es una
%%%%%%%%         $N(0,1)$ entonces $F_{Z}(-x)=1-F_{Z}(x)$
%%%%%%%%         * $Z=\frac{X-\mu}{\sigma}$ es una v.a. $N(0,1)$ y
%%%%%%%%              $X=\sigma Z+\mu$ es una $N(\mu,\sigma^2)$ donde $Z$ es la
%%%%%%%%              normal estándar.
%%%%%%%%
%%%%%%%%          \end{enumerate}
%%%%%%%%



%%%%%%%%
%%%%%%%%
%%%%%%%% Diremos que una v.a. continua $X$ tiene
%%%%%%%%distribución normal con parámetros $\mu$ y  $\sigma^2$ a una variable aleatoria que tenga
%%%%%%%%por función de densidad :
%%%%%%%%$$f(x)={1\over{\sqrt{2\pi}\sigma}} {e\vphantom{A}}^{\left(-{1\over
%%%%%%%%2}{\left({x-\mu}\over{\sigma}\right)}^2\right)}$$






\begin{figure}[h]
\begin{center}
\begin{tabular}{cc}
 \includegraphics[scale=0.75]{densidadgaussiana}
&
\includegraphics[scale=0.75]{distribuciongaussiana}\\ a) & b) \end{tabular}
\end{center}
       \caption{Gráficas de la función de densidad (a)  y de la  función de distribución (b) de una v.a. $N(0,1)$.}
        \end{figure}


%%%%%%%%\begin{figure}
%%%%%%%%\begin{center}
%%%%%%%%\includegraphics{normal.eps}
%%%%%%%%\end{center}
%%%%%%%%\caption{Curva de Gauss con $\mu=0$ y $\sigma=1$ }
%%%%%%%%\end{figure}






Su función de distribución es, como sabemos :
$$F(x)=\int_{-\infty}^{x} {1\over{\sqrt{2\pi}\sigma}}
{e\vphantom{A}}^{-{1\over 2}{\left({t-\mu}\over{\sigma}\right)}^2} dt$$

Que no tiene ninguna expresión algebraica "decente". Es por esta razón, y  por comodidad,
que esta función está tabulada.



Cuando una variable tiene distribución normal con parámetros $\mu,\sigma$ la denotamos por
$X\equiv N(\mu,\sigma^2)$





\subsubsection{Resumen v.a con distribución normal, $N(\mu,\sigma^2)$}

\scriptsize
\begin{tabular}{|c|c|c|c|c|}
\hline \begin{tabular}{c} Valores\\ admisibles.\end{tabular} & $f_{X}(x)$ & $F_X(x)=P(X\leq
X)=$ &
 $E(X)$ & $Var(X)$\\\hline & & & &\\
 $D_X=\mathbb{R}$ & $=\frac1{\sqrt{2\pi}\sigma}
          e^{\frac{-(x-\mu)^2}{2\sigma^2}}\mbox{ para todo }x\in \mathbb{R}$ & Tabulada la
          $N(0,1)$ & $\mu$ & $\sigma^2$ \\& & & &\\ \hline
\end{tabular}
\normalsize






\subsubsection{Propiedades de la distribución normal.} La función de densidad de la
distribución normal tiene las siguientes propiedades:
\begin{enumerate}[a)]
* $f$ es continua
* $\int_{-\infty}^{+\infty} {1\over{\sqrt{2\pi}\sigma}} {e\vphantom{A}}^{-{1\over
2}{\left({x-\mu}\over{\sigma}\right)}^2} dx =1$ ( propiedad de todas las densidades).
* $f(\mu+x)=f(\mu-x)$ y $F(x+\mu)=1-F(\mu-x)$ para todo $x\in \cal{R}$
* $\lim\limits_{x\to+\infty}f(x)=\lim\limits_{x\to-\infty}f(x)=0$
es decir tiene asíntota horizontal a derecha e izquierda.
* $f$ es estrictamente creciente si $x<\mu$ y decreciente si $x>\mu$.
* Alcanza el máximo en $x=\mu$ y en este punto vale $f(\mu)=\frac1{\sqrt{2\pi}\sigma}$
* Tiene dos puntos de inflexión en $x=\mu+\sigma$ y en $x=\mu-\sigma$.
\end{enumerate}





## Transformaciones lineales de variables aleatorias normales


<l class="prop"> Propiedad: transformación lineal la distribución  normal </l>

Sea $X\equiv N(\mu,\sigma^2)$  entonces la variable $Y=a X+b$ con
$a\not=0,b\in\cal{R}$ tiene distribución $N(a\mu+b, a^2 \sigma^2)$

En particular si  $X\equiv N(\mu,\sigma^2)$, tomando $a=\frac1{\sigma}$ y $b=
\frac{-\mu}{\sigma}$ la v.a. 

$$Z={{X-\mu}\over {\sigma}}$$
se distribuye $N(0,1)$.



Esta propiedad es muy importante, ya que utilizándola sólo necesitaremos tabular la
$N(0,1)$. A la función de distribución de una $Z\equiv N(0,1)$ la llamaremos $F_Z$  y a una
normal $N(0,1)$  se le denomina normal estándar. Por lo tanto si
$F_X(x)=F_Z(\frac{x-\mu}{\sigma})$.

## Cálculos con la distribución normal

<l class="example"> Cálculos con la distribución  normal </l>

Dada  $Z\equiv N((0,1)$ entonces $F_{Z}(x)=1-F_{Z}(-x)$.

<div class="example-sol">

* Dado $\delta>0$, $P(-\delta\leq Z \leq
\delta)=F_{Z}(\delta)-F_{Z}(-\delta)=F_Z(3)-(1-F_Z(\delta))=2 F_Z(\delta)-1$
* $P(-4\leq Z \leq 4)=F_{Z}(4)-F_{Z}(-4)=2 F_Z(4)-1$
* $P(-2\leq Z \leq 2)=F_{Z}(2)-F_{Z}(-2)=2 F_Z(2)-1$
* $P(Z\leq -2)=F_Z(-2)=1-F_Z(2)$
* $P( Z \leq 2)=F_{Z}(2)$
* $P( Z \geq 2)=1-P(Z<2)=1-F_{Z}(2)$
* $P( Z > 2)=1-P(Z\leq 2)=1-F_{Z}(2)$
* $P( Z = 2)=0$
* $P( Z \geq -2)=1-P(Z< -2)=1-F_{Z}(-2)=1-(1-F_Z(2))=F_Z(2).$
</div>




    Resumiendo podemos utilizar las siguientes propiedades, $X\equiv N(\mu,\sigma)$
    
    *  $Z$ es su variable tipificada, es decir,
    $Z=\frac{X-\mu}{\sigma}\equiv N(0,1)$ entonces:

    $$P(X\leq x)=P(\frac{X-\mu}{\sigma}\leq
    \frac{x-\mu}{\sigma})=F_{Z}(\frac{x-\mu}{\sigma})$$

   *  Cuando tengamos un intervalo
    $$P(a<X<b)=P(\frac{a-\mu}{\sigma}<\frac{X-\mu}{\sigma}<\frac{b-\mu}{\sigma})=$$

    $$=P(\frac{a-\mu}{\sigma}<Z<\frac{b-\mu}{\sigma})=F_{Z}(\frac{b-\mu}{\sigma})-
    F_{Z}(\frac{a-\mu}{\sigma})$$
    * Si $\delta>0$ $P(\mu-\delta\leq X \leq
\mu+\delta)=2 F_Z(\frac{\delta}{\sigma})-1$


## Ejemplo
Sea $X$ una normal com media $2$ y varianza $4$, entonces

*  $P(1< X< 2)= P(\frac{1-2}2<\frac{X-2}2<\frac{2-2}2)=P(\frac{-1}2<Z<0)=F_{Z}(0)-F_{Z}(-0.5)=\frac12-1+F_{Z}(0.5).$
* $P(X>3)=P(\frac{X-2}2>\frac{3-2}2)=P(Z>0.5)=1-F_{Z}(0.5).$





