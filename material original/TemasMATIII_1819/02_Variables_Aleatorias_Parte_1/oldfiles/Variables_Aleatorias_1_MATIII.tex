%\documentclass[12pt,t]{beamer}
% \documentclass[t]{beamer}
\documentclass[handout]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{pgfpages}
\usepackage{pgffor}
%\pgfpagesuselayout{4 on 1}[a4paper,landscape]

%\pagestyle{empty} % descomentar para impresión muy blanca

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\decimalpoint
\usepackage{verbatim}
\usepackage{hyperref}
%\hypersetup{colorlinks=false,linkbordercolor=red,linkcolor=green,pdfborderstyle={/S/U/W 1}}
%\hypersetup{colorlinks=true,linkbordercolor=red,linkcolor=green,pdfborderstyle={/S/U/W 1}}
\hypersetup{colorlinks=true,linkcolor=blue,pdfborderstyle={/S/U/W 1}}

\usepackage{amsfonts,amssymb,amsmath,amsthm, wasysym}
\usepackage{listings}
%\usepackage[T1]{fontenc}        
\usepackage{pgf}
\usepackage{epsdice}
\usepackage{pgfpages}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,plotmarks,backgrounds,trees,positioning}
\usetikzlibrary{decorations.pathmorphing,calc,snakes}
%\usepackage{marvosym}
%
\usetheme[hideothersubsections,left]{Marburg}
%\usetheme[hideothersubsections,left]{Madrid}
%\usetheme[hideothersubsections,left]{Dresden}
%\usetheme{Darmstadt}
\usecolortheme{sidebartab}
\useinnertheme[shadow]{rounded}

% \useoutertheme[footline=empty,subsection=true,compress]{infolines}
% \useoutertheme[footline=empty,subsection=true,compress]{miniframes}
% \usefonttheme{serif}

\setbeamertemplate{caption}[numbered]
%\setbeamertemplate{navigation symbols}{}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
    
    \setbeamercolor{footline}{fg=blue}
\setbeamerfont{footline}{series=\bfseries}
}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\renewcommand{\emph}[1]{{\color{red}#1}}

%\newtheorem{theorem}



\setbeamertemplate{frametitle}
{\begin{centering}
\medskip
\color{blue}
\textbf{\insertframetitle}
\medskip
\end{centering}
}
\usecolortheme{rose}
\usecolortheme{dolphin}
\mode<presentation>


\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\MM}{\mathcal{M}}
%\newcommand{\dbinom}{\displaystyle\binom}

\newcommand{\limn}{{\displaystyle lim_{n\to\infty}}}

%\renewcommand{\lim}{\displaystyle \mathrm{lim}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\def\tendeix{{\displaystyle\mathop{\longrightarrow}_{\scriptscriptstyle
n\to\infty}}}

\newcommand{\matriu}[1]{\left(\begin{matrix} #1 \end{matrix}\right)}

% \newcommand{\qed}{\hbox{}\nobreak\hfill\vrule width 1.4mm height 1.4mm depth 0mm
%     \par \goodbreak \smallskip}
%
% %

\theoremstyle{plain}
\newtheorem{teorema}{Teorema}
%\newtheorem{prop}{Proposición}
\newtheorem{prop}{Propiedades}
\newtheorem{cor}{Corolario}
\theoremstyle{definition}
\newtheorem{ejemplo}{Ejemplo}
\newtheorem{definicion}{Definición}
\newtheorem{obs}{Observación}

\newcounter{seccions}
\newcommand{\seccio}[1]{\addtocounter{seccions}{1}
\medskip\par\noindent\textbf{\theseccions.
#1}\smallskip\par }

\newcommand{\EM}{\Omega}
\newcommand{\PP}{\mathcal{P}}

\title[\red{Matemáticas III GINF}]{}
\author[]{R. Alberich}
\date{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\beamertemplatedotitem

\lstset{backgroundcolor=\color{green!50}}
\lstset{breaklines=true}
\lstset{basicstyle=\ttfamily}


\section{Variables Aleatorias Parte I}

\begin{frame}
\vfill
\begin{center}
\gray{\LARGE Variables Aleatorias Parte I}
\end{center}
\vfill
\end{frame}
\section{Variables aleatorias}

\begin{frame}
\frametitle{Introducción}
\begin{itemize}
\item Hasta ahora nuestros sucesos han sido de varios tipos: $\{C,+\}$ en
la moneda, nombres de periódicos, ángulos en una ruleta, número de
veces que sale cara en el lanzamiento de una moneda etc\ldots.
\item Necesitamos estandarizar de alguna manera todos estos sucesos. Una
solución es asignar a cada suceso un cierto conjunto de
números reales, es decir, convertir todos los sucesos en
\emph{sucesos de números reales} para trabajar con ellos de forma
unificada.
\item Para conseguirlo utilizaremos  unas funciones que
transformen los elementos del espacio muestral en números; esta funciones son las
variables aleatorias.
\end{itemize}
\end{frame}



\subsection{Variables aleatorias}

\begin{frame}
\frametitle{Definición de variable aleatoria}
 Comenzaremos dando una definición práctica de  variable aleatoria.
\begin{definicion}
Definición práctica de \emph{variable aleatoria} 
%\footnote{Formalmente dado un espacio de probabilidad $(\Omega,\mathcal{F},P)$ y
%el espacio probabilizable\newline $(\RR,\mathcal{B}(\RR))$ una aplicación $X:\Omega\to\RR$ es una v.a. si y
%sólo si $X^{-1}(B)\in \mathcal{F}$ para todo $B\in\mathcal{B}(\RR)$.}
%una variable
(v.a.) es una aplicación que toma valores numéricos determinados por el resultado de un experimento aleatorio
$$X:\Omega\to\RR$$
\end{definicion}
% \end{frame}
% 
% 
% \begin{frame}
\textbf{Notación}:
   \begin{itemize}
       \item Normalmente representaremos las v.a. por letras
       mayúsculas $X,Y,Z$\ldots
       \item Los valores que ``\emph{toman}'' las v.a. los
       representaremos por letras minúsculas (las mismas en principio)
       $x,y,z\ldots$
    \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Ejemplo}
Lanzamos un dado convencional de parchís el espacio muestral del experimento es

$$\Omega=\{\epsdice{1}, \epsdice{2}, \epsdice{3}, \epsdice{4},  \epsdice{5},
\epsdice{6}\}$$

y  una v.a $X:\Omega\to\RR$
sobre este espacio queda definida por 

\begin{equation*}
\begin{split}
X(\epsdice{1})&=1,X( \epsdice{2})=2,X( \epsdice{3})=3,\\
X(\epsdice{4})&=4,X( \epsdice{5})=5,X( \epsdice{6})=6.
\end{split}
\end{equation*}

 Ahora el suceso $A=\{ \epsdice{2}, \epsdice{4}, \epsdice{6}\}$, es decir ``salir
número par'', es equivalente a $\{X=2,X=4,X=6\}$.

El suceso $B=\{ \epsdice{1},\epsdice{2}, \epsdice{3}\}$, es decir ``salir un número
inferior o igual a $3$'' es  en términos de la v.a. $\{X=1,X=2,X=3\}$ o también $\{X\leq
3\}$.
\end{frame}

\begin{frame}
\frametitle{Ejemplo}
Consideremos el experimento lanzar una anilla al cuello de una botella. Si acertamos a
ensartar la anilla en la botella el resultado del experimento es \emph{éxito} y
\emph{fracaso} en caso contrario. 

El espacio muestral asociado a este experimento será
$\Omega=\{\mbox{éxito, fracaso}\}$. Construyamos la siguiente variable aleatoria:

$$X:\{\mbox{éxito, fracaso}\}\to\RR$$

definida por 

$$X(\mbox{éxito})=1 \mbox{ y } X(\mbox{fracaso})=0.$$

\end{frame}


\subsection{Tipos de variables aleatorias}
\begin{frame}

Hay dos tipos fundamentales de variables aleatorias, las discretas y las continuas.
Damos a continuación una definición informal
% \footnote{
% \begin{itemize}
% \item La distinción entre variables aleatorias discretas y continuas es teóricamente más
% sofisticada. En la práctica serán discretas aquellas variables a las que podamos asignar
% probabilidades a los sucesos elementales y queden así descritas, siendo continuas las
% ``restantes''. En los problemas que trataremos en este curso esto será casi siempre así.
%       \item  Hay variables aleatorias mixtas que, no por ser menos frecuentes, tienen menos
% importancia.  Por ejemplo el tiempo que está ocupado un procesador atendiendo los trabajos
% que llegan en una hora, si llega al menos un trabajo nos dará una v.a. continua pero si no
% llega ningún trabajo el tiempo que está ocupado es cero en ese punto, que posiblemente
% tenga probabilidad mayor que cero, la v.a. tiene un comportamiento discreto. Otro ejemplo
% más artificial es el siguiente juego: lanzar un dado al aire y  si sale numero par anotar
% el resultado mientras que  si sale impar lanzar un dardo a una diana y anotar la distancia
% al centro. \end{itemize}}
de estos tipos.
\begin{definicion}
    \begin{enumerate}[a)]
    \item Una variable aleatoria es \emph{discreta} si sólo puede tomar una cantidad numerable de valores con probabilidad
     positiva.
    \item La variables aleatorias \emph{continuas}  toman  valores en intervalos.
    \item Variables aleatorias \emph{mixtas}; con una parte discreta y otra continua.
\end{enumerate}
\end{definicion}
\end{frame}

\begin{frame}
\frametitle{Ejemplo}
      Son variables \emph{aleatorias discretas}:
      \begin{itemize}
          \item  Número de artículos defectuosos en un cargamento.
          \item  Número de clientes que llegan a una ventanilla de un
          banco en una hora.
          \item  Número de errores detectados en las cuentas de una
          compañía.
          \item  Número de reclamaciones de una póliza de un seguro
          médico.
      \end{itemize}
\end{frame}

\begin{frame}
       Son variables \emph{aleatorias continuas}:
      \begin{itemize}
          \item  Renta anual de una familia.
          \item Cantidad de petróleo importado por un país
          \item  Variación del precio de las acciones de una compañía
          de telecomunicaciones.
          \item  Porcentaje de impurezas en un lote de productos
          químicos.
      \end{itemize}
%
\end{frame}

\section{Distribuciones de probabilidad para v.a. discretas.}

\begin{frame}
\frametitle{Distribuciones de probabilidad para v.a. discretas.}
\begin{itemize}
\item Pasamos ahora a describir el comportamiento  de la v.a. 
Para ello utilizaremos distintas
funciones que nos darán algunas probabilidades de la variable aleatoria.
\item En el caso discreto estas funciones son la de probabilidad, y  la función de distribución o de probabilidad acumulada.
\item En el caso discreto la función de probabilidad es la que nos da las probabilidades de los
sucesos elementales de la v.a. que definimos a continuación.
\end{itemize}
\end{frame}

\subsection{Función de probabilidad de una variable aleatoria discreta}

\begin{frame}

\begin{definicion}
La \emph{función de probabilidad} (\textsl{probability mass function} o incluso abusando de notación \textsl{probability density function})
%\footnote{También llamada función de cuantía o de masa
% de probabilidad,  inglés "\emph{ probability mass function}" por lo que se
% abrevia con el acrónimo \emph{pmf}, recordar esto cuando busquéis comandos en paquetes
%estadísticos}
de una variable aleatoria discreta $X$ a la que denotaremos por $P_{X}(x)$
está definida por
%\footnote{ Concretamente $P(X=x)=P(X^{-1}(x))$.} 

$$P_{X}(x)=P(X=x)$$

es decir la probabilidad de que $X$ tome el valor $x$. Si $X$ no asume ese valor entonces
$P_{X}(x)=0$.

El conjunto $$D_X=\{ x\in\RR \mid P_X(x)>0\}$$ recibe el nombre de
\emph{dominio} de la v.a. y son los valores posibles de esta variable. 


En el caso discreto lo más habitual es que $X(\Omega)=D_X$.
\end{definicion}
\end{frame}



\begin{frame}
\frametitle{Ejemplo}
Lanzamos un dado de parchís una vez, en esta ocasión representaremos los
sucesos elementales por el número de puntos de la cara obtenida, tenemos que
$$\Omega=\{1,2,3,4,5,6\}$$ 
y la variables aleatoria $X:\Omega\to \RR$ viene definida por

$$X(i)=i\mbox{ para } i=1,2,3,4,5,6.$$
  
Supongamos que el dado está bien balanceado. Entonces
$$P_{X}(1)=P_{X}(2)=P_{X}(3)=P_{X}(4)=P_{X}(5)=P_{X}(6)=\frac{1}{6}$$
Concretamente:
$$
P_{X}(x)=
  \left\{
  \begin{array}{ll}
   \frac{1}{6} & \mbox{si } x=1,2,3,4,5,6\\
  0 & \mbox{en otro caso }
  \end{array}
  \right.
  $$
  
  Su dominio es $$D_X=\{1,2,3,4,5,6\}.$$
%

\end{frame}

\begin{frame}

\frametitle{Ejemplo}
Sea $X$ la v.a. asociada al lanzamiento de una moneda. Su espacio muestra es  $\Omega=\{c,+\}$, la v.a. queda definida por:

$$X(\omega)=\left\{\begin{array}{ll} 1 & \mbox{si } \omega=c \\
0 & \mbox{si }\omega=+\end{array}\right.$$

entonces su función de probabilidad es:

$$P_{X}(x)=P(X=x)=\left\{\begin{array}{ll} \frac{1}{2} & \mbox{si } x=0,1\\
0 & \mbox{en otro caso}\end{array}\right.$$


Finalmente su dominio es $D_X=\{0,1\}.$
\end{frame}



\begin{frame}
\frametitle{Ejemplo} 
Tenemos una urna con tres bolas rojas,una negra y dos blancas. Realizamos una extracción y observamos el color de la bola entonces un espacio muestral es
$$\Omega=\{roja, blanca, negra\}.$$ 

Una variable aleatoria asociada al experimento es:

$$X(\omega)=\left\{\begin{array}{ll} 1 & \mbox{si } \omega=roja  \\
2 & \mbox{si }\omega=negra \\ 3 & \mbox{si } \omega=blanca \end{array}\right.$$

entonces

$$P_{X}(x)=\left\{\begin{array}{ll} \frac{3}{6} & \mbox{si } x=1\\
\frac{1}{6} & \mbox{si } x=2\\ \frac{2}{6} & \mbox{si } x=3\\ 0 & \mbox{en otro
caso}\end{array}\right.$$

El dominio de $X$ es $D_X=\{1,2,3\}.$

\end{frame}

\subsection{Propiedades de la función de probabilidad}
\begin{frame}

\frametitle{Propiedades de la función de probabilidad.}
 Sea $X$ una v.a. discreta $X:\Omega:\to\RR$ con dominio $D_X$. Su función de probabilidad $P_{X}$ verifica las siguientes propiedades:
\begin{enumerate}[a)]
\item $0\leq P_{X}(x)\leq 1$ para todo $x\in\RR$
\item $\sum\limits_{x\in D_X} P_{X}(x)=1$
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%PPPPPPPPPPPPPP

\begin{frame}
\frametitle{Ejemplo}
Lanzamos al aire tres veces, de forma independiente, una
moneda perfecta. El espacio muestral de este experimento es
$$\Omega=\{ccc,cc+,c+c,+cc,c++,+c+,++c,+++\}$$ (expresados en orden
de aparición).

Este espacio tiene todos los sucesos elementales
equiprobables. 


Consideremos la variable aleatoria asociada a este experimento $X=$ número de caras en los tres lanzamientos. 
\end{frame}

\begin{frame}
\frametitle{Ejemplo}
Entonces
    $$\begin{array}{l}
P(X=0)=P(\{+++\})=\frac{1}{8}\\ P(X=1)=P(\{c++,+c+,++c\})=\frac{3}{8}\\
    P(X=2)=P(\{cc+,c+c,+cc\})=\frac{3}{8}\\
    P(X=3)=P(\{ccc\})=\frac{1}{8}
    \end{array}$$

    La función de probabilidad de $X$ es:
    $$P_{X}(x)=\left\{\begin{array}{ll} \frac{1}{8} & \mbox{si } x=0,3\\
\frac{3}{8} & \mbox{si } x=1,2\\ 0 & \mbox{en otro caso}\end{array}\right.$$

Efectivamente  $$\sum_{x=0}^3 P_X(x)= \frac{1}{8}+\frac{3}{8}+\frac{3}{8}+\frac{1}{8}=1.$$


\end{frame}
\section{Función de distribución de variables aleatorias}

\begin{frame}
\frametitle{Función de distribución de variables aleatorias}

\begin{definicion}
La función de \emph{distribución de probabilidad} (acumulada) de la v.a. $X$ (de cualquier tipo;
discreta o continua) $F_{X}(x)$ representa la probabilidad de que $X$  tome un menor o igual que 
 $x$ es decir
$$F_{X}(x)=P(X\leq x)$$
\end{definicion}


Esta función también se denomina función de \emph{distribución de
probabilidad o simplemente función de distribución} de una v.a., y en inglés
\emph{cumulative distribution function} por lo que se abrevia con el acrónimo \emph{cdf}.
\end{frame}

\begin{frame}
\frametitle{Propiedades}

Sea $X$ una v.a. y $F_{X}$ su función
de distribución:
\begin{enumerate}[1)]
    \item $P(X>x)=1-P(X\leq x)=1-F_{X}(x)$
    \item Sea a y b tales que $a<b$, $P(a<X\leq b)=P(X\leq b)-P(X\leq a)=F_{X}(b)-F_{X}(a)$
\end{enumerate}
\end{frame}
\begin{frame}
\textit{Demostración:}
\begin{enumerate}[1)]
    \item $\overline{\left\{X>x\right\}}=\left\{X\leq x\right\}$.
    $P(X>x)=1-P(\overline{\left\{X>x\right\}})=1-P(X\leq x)=1-F_{X}(x)$
    \item $\left\{a< X \leq b\right\}= \left\{X\leq b\right\}-\left\{X\leq
    a\right\}$
    $P(a<X\leq b)=P(\left\{X\leq b\right\}-\left\{X\leq
    a\right\})=P(\left\{X\leq b\right\})-P(\left\{X\leq
   a\right\})=F_{X}(b) -F_{X}(a)$.
\end{enumerate}
\end{frame}

\begin{frame}[fragile]
\frametitle{Propiedades}
%\begin{prop}
       Sea $F_{X}$ la función de distribución  de una  v.a. $X$ entonces:
\begin{enumerate}[a)]
\item  $0\leq F_{X}(x)\leq 1$.
\item La función $F_{X}$ es no decreciente.
\item La función $F_{X}$ es continua por la derecha.
\item Si denotamos por $F_X(x_0^{-})=\displaystyle \lim_{x\to x_0^{-}} F(x)$,
entonces se cumple que $P(X< x_0)=F_X(x_0^{-})$ y que $P(X=x_0)=F_X(x_0)-F_X(x_0^{-})$.
\item Se cumple que $\displaystyle \lim_{x\to\infty} F_{X}(x)=1$; $\displaystyle \lim_{x\to-\infty}F_{X}(x)=0$.
\item  Toda función $F$ verificando las propiedades anteriores es función de distribución de alguna v.a. $X$.
\item $P(X>x)=1-F_{X}(x)$
\item Dados $a,b\in \RR$ con $a<b$ $$P(a<X\leq b)=F_{X}(b)-F_{X}(a).$$
\end{enumerate}
%\end{prop}
\end{frame}



\begin{frame}

\frametitle{Propiedades}

En las propiedades anteriores no se pueden cambiar en general las desigualdades de
estrictas o no estrictas, veamos que propiedades tenemos cuando se cambian estas
desigualdades.
%\begin{prop}
           Sea $F_{X}$ una función de distribución de la v.a. $X$ y denotamos
           por $F_{X}(x_{0}^{-})=\displaystyle \lim_{x\to x_{0}^{-}} F_{X}(x)$, entonces.
           \begin{enumerate}[a)]
           \item $P(X=x)=F_{X}(x)-F_{X}(x^{-})$
           \item $P(a< X< b)=F_{X}(b^{-})-F_{X}(a)$
           \item $P(a\leq X< b)=F_{X}(b^{-})-F_{X}(a^{-})$
           \item $P(X<a)=F_{X}(a^{-})$
           \item $P(a\leq X\leq b)=F_{X}(b)-F_{X}(a^{-})$
           \item $P(X\geq a)=1-F_{X}(a^{-})$
        \end{enumerate}
%\end{prop}
\end{frame}


\begin{frame}

\frametitle{Propiedades}
\begin{enumerate}[a)]
\item Si  $F_X$ es continua en $x$ se tiene que $P(X=x)=0$.
Así que si la v.a. es continua $P(X\leq a)=P(X< a)+P(X=a)=P(X<a)$ y propiedades similares.
\item  Sea $X$ una variable aleatoria discreta que con dominio $D_X$ y
que tiene por función de probabilidad $P_{X}(x)$ entonces su función de distribución
$F_{X}(x_0)$ es
$$F_{X}(x_0)=\sum_{x\leq x_{0}} P_{X}(x)$$

donde $\sum_{x\leq x_{0}}$ indica que sumamos todos los $x \in D_X$ tales que $x\leq
x_{0}$
\end{enumerate}
\end{frame}


\begin{frame}

\textbf{Demostración:}


\begin{enumerate}[a)]
\item Si $X$ es continua $$P(X=a)=F(a)-F(a^{-})=F(a)-F(a)=0$$
por lo tanto 
\begin{equation*}
\begin{split}
P(X\leq a)=& P(X<a)+P(X=a)\\
=& P(X<a)+0= P(X<a).
\end{split}
\end{equation*}
\item 
\begin{eqnarray*}
F_{X}(x_{0})&=P(X\leq x_{0})=P\left(\bigcup_{x\leq
x_{0}; x\in D_X} \{x\}\right)\\
&=\sum_{x\leq x_{0}}P(X=x)= \sum_{x\leq x_{0}}P_{X}(x).
\end{eqnarray*}
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Ejemplo}
   En el experimento del dado se tiene que:

   $$P_{X}(x)=\left\{\begin{array}{ll}
   \frac{1}{6} & \mbox{si } x=1,2,3,4,5,6\\
   0 & \mbox{en el resto de casos}\end{array}\right.,$$

por lo tanto

   $$F_{X}(x)=P(X\leq x)=\left\{\begin{array}{ll}
   0 & \mbox{si } x<1\\
   \frac{1}{6} &\mbox{si } 1\leq x<2\\
   \frac{2}{6} &\mbox{si } 2\leq x<3\\
   \frac{3}{6} &\mbox{si } 3\leq x<4\\
   \frac{4}{6} &\mbox{si } 4\leq x<5\\
   \frac{5}{6} &\mbox{si } 5\leq x<6\\
   1 &\mbox{si } 6\leq x\end{array}\right.$$
\end{frame}

\begin{frame}
\frametitle{Ejemplo}

Calculemos más detalladamente algún valor de $F_{X}$, por ejemplo:

\begin{equation*}
\begin{split}
F_{X}(3.5)= & P(X\leq 3.5)=  P(\{X=1\}\cup\{X=2\}\cup \{X=3\})\\
=& P(\{X=1\})+P(\{X=2\})+P(\{X=3\})\\=& 
\frac{1}{6}+\frac{1}{6}+\frac{1}{6}=\frac{3}{6}
=\frac{1}{2},
\end{split}
\end{equation*}


o de otra forma

  
\begin{equation*}
\begin{split}
F_{X}(3.5)=&\sum_{x\leq 3.5} P_X(x)=\sum_{x=1}^3 P(X=x)\\=&\sum_{x=1}^3 \frac{1}{6}= 3 \cdot
   \frac{1}{6}=\frac{1}{2}.
\end{split}
\end{equation*}

\end{frame}

\begin{frame}



\frametitle{Propiedades de la función de distribución}

Sea $X$ una variable con función de distribución $F_{X}$ entonces:
\begin{enumerate}[a)]
\item $0\leq F_{X}(x)\leq 1$ para todo $x$
\item Si $x<x'$ entonces $$F_{X}(x)\leq F_{X}(x').$$
Es una función creciente, no necesariamente estrictamente creciente.
\item $\displaystyle \lim_{x\to -\infty}F_{X}(x)=0$ y $\displaystyle \lim_{x\to +\infty}F_{X}(x)=1$
\item Es continua por la derecha $\displaystyle \lim_{x\to x_{0}^{+}}F_{X}(x)=F_{X}(x_{0})$.
\end{enumerate}
\end{frame}

\section{Momentos de variables aleatorias discretas}

\begin{frame}
\frametitle{Momentos de variables aleatorias discretas}

\begin{itemize}
\item Al igual que en  estadística descriptiva  utilizamos distintas medidas para
resumir los valores centrales y para medir la dispersión de una muestra, podemos definir
las correspondiente medidas para variables aleatorias.

\item Estas medidas se les suele añadir
el adjetivo poblacionales mientras que a las que provienen de la muestra se las adjetiva
como muestrales.
\end{itemize}

Por ejemplo  podemos buscar un valor que resuma toda la variable.
Este valor es el que ``\emph{esperamos}'' que se resuma la v.a. o esperamos que las
realizaciones de la v.a. queden cerca de él. Veamos su definición formal.

\end{frame}

\subsection{Esperanza para variables aleatorias discretas}

\begin{frame}


\begin{definicion}
    El valor \emph{esperado o esperanza} (\textsl{expected value} en inglés) $E(X)$ de una v.a. discreta $X$, se define como
    $$E(X)=\sum_{x\in X(\Omega)} x P_{X}(x)$$

    En ocasiones se le domina \emph{media} (\textsl{mean} en inglés \textsl{mitjana} en catalán) poblacional o simplemente media y muy frecuentemente se la denota
    $\mu_{X}=E(X)$ o simplemente $\mu=E(X)$.
    \end{definicion}
\end{frame}

\subsubsection{Relación de la esperanza para variables aleatorias
discretas con la media aritmética}


\begin{frame}
\frametitle{Interpretación de la media aritmética}

Supongamos que lanzamos un dado $n$ veces y obtenemos unas frecuencias absolutas $n_{i}$
para el resultado $i$ con $i=1,\ldots,6$. Sea $X$ la v.a. que nos representa el valor de
una tirada del dado.

Calculemos la media aritmética (o media muestral) de los datos

$$\overline{x}=\frac{1\cdot n_{1}+2\cdot  n_{2}+3\cdot  n_{3}+4\cdot  n_{4}+5\cdot  n_{5}+6 \cdot 
n_{6}}{n}=\sum_{x=1}^{6} x \frac{n_{x}}{n}.$$
\end{frame}



\begin{frame}
\frametitle{Interpretación de la media aritmética}

Si $n\to \infty$ se tiene que  
$$\lim_{n\to \infty} \frac{n_{x}}{n}=P_{X}(x).$$

Por lo tanto

$$E(X)=\displaystyle \lim_{n\to\infty}\sum_{x=1}^{6}x \frac{n_{x}}{n}.$$

Entonces el valor esperado en una v.a. discreta puede entenderse como el valor promedio que
tomaría una v.a. en un número grande de repeticiones.
\end{frame}


\begin{frame}

\frametitle{Ejemplo}
    Sea $X$= número  de erratas en una página de un texto con dominio $D_X=\{0,1,2\}$, y resulta que

    $$P(X=0)=0.42, P(X=1)=0.4, P(X=2)=0.18.$$
    
    entonces
    
    $$E(X)=0\cdot 0.42+ 1\cdot 0.4 + 2 \cdot 0.18=0.76.$$

    Elegida una página del texto al azar esperamos encontrar $0.76$
    errores.

\end{frame}


\subsection{Esperanzas de funciones de variables aleatorias discretas}

\begin{frame}
\frametitle{Ejemplo}
Supongamos que en el ejemplo anterior el editor nos paga $2$ euros por cada página que
encontremos con $1$ error y $3$ euros por cada página con  dos errores (y nada por las
páginas correctas) ¿Cuánto esperamos cobrar si analizamos una página?

$$0\cdot 0.42 + 2\cdot 0.4 + 3\cdot 0.18=1.34$$
\end{frame}

\begin{frame}
\frametitle{Esperanzas de funciones de variables aleatorias discretas}
\begin{definicion}
Sea $X$ una v.a. discreta con función de probabilidad $P_{X}$ y de distribución
$F_{X}$. Entonces el \emph{valor esperado de una función} $g(x)$ es :

$$E(g(X))=\sum_{x}g(x) P_{X}(x).$$
\end{definicion}
\end{frame}

\begin{frame}
\frametitle{Propiedades de los valores esperados}
%La demostración de las siguientes propiedades se deja como ejercicio.

\begin{enumerate}[a)]
\item $E(k)=k$ para cualquier constante $k$.
\item Si $a\leq X\leq b$ entonces $a\leq E(X)\leq b$.
\item Si $X$ es una v.a. discreta que toma valores enteros no negativos entonces
$E(X)=\sum_{x=0}^{+\infty}(1- F_X(x)).$
\end{enumerate}

\end{frame}



\begin{frame}
\frametitle{Ejemplo}
Supongamos que estamos sentados delante de nuestro ordenador con un amigo y
le decimos que en dos minutos podemos programar una paleta  para poner colores a unos
gráficos. 

Queremos la que paleta tenga dos botones con las opciones color rojo y color azul.
Como hemos programado a gran velocidad resulta que el programa tiene un error; cada vez que
se abre la paleta los colores se colocan al azar (con igual probabilidad) en cada botón,
así que no sabemos en que color hemos de pinchar. 

Además, como nos sobraron $15$ segundos
para hacer el programa y pensando en la  comodidad del usuario, la paleta se cierra después de haber seleccionado  un
color y hay que volverla a abrir de nuevo.

La pregunta es ¿cuál es el valor esperado del
número de veces que hemos pinchar el botón de color azul antes de obtener este color?
\end{frame}


\begin{frame}
\frametitle{Ejemplo}

Llamemos $X$ al número de veces que pinchamos en el botón azul (y nos sale rojo) hasta
obtener el primer azul. La variable $X$ toma valores en los enteros no negativos. Su
función de probabilidad queda determinada por

$$P_X(x)=P(X=x)=P(\stackrel{x \mbox{veces}}{\overbrace{rojo, rojo,\ldots,rojo},azul})
=\left(\frac{1}{2}\right)^{x+1}$$
\end{frame}


\begin{frame}


\frametitle{Propiedades de las series geométricas}
Recordemos conceptos básicos de las  series geométricas.
\begin{itemize}
\item Una progresión geométrica es una sucesión de la  forma  $$r^0, r^1,\ldots,r^n,\ldots.$$
El valor $r$ recibe el nombre de razón de la progresión geométrica. 
\item La serie geométrica es la suma de todos los
valores de la progresión geométrica $\displaystyle\sum_{k=0}^{+\infty} r^k$.
\item Las sumas parciales desde el término $n_0$ al $n$ de una progresión geométrica son
 $\sum_{k=n_0}^n r^k=\frac{r^{n_0}- r^n r}{1-r}.$

\item Si $|r|<1$ la serie geométrica es convergente y $\displaystyle\sum_{k=0}^{+\infty }
r^k=\frac{1}{1-r}$. En el caso en que se comience en $n_0$ se tiene que
$\displaystyle\sum_{k=n_0}^{+\infty} r^k=\frac{r^{n_0}}{1-r}$.
\end{itemize}

\end{frame}


\begin{frame}

\frametitle{Conceptos básicos de series geométricas}
\begin{itemize}
\item  Si $|r|<1$  también son convergentes las derivadas, respecto de $r$,
 de la serie geométrica y convergen a la derivada correspondiente. Así tenemos que
\begin{align*}
\begin{split}
\left(\sum_{k=0}^{+\infty} r^k\right)'= & \sum_{k=1}^{+\infty}k
r^{k-1}\\
=& \left(\frac{1}{1-r}\right)'=\frac{1}{(1-r)^2}.\\
\left(\sum_{k=0}^{+\infty} r^k\right)''=& \sum_{k=2}^{+\infty}k (k-1)
r^{k-2}\\ =&\left(\frac{1}{1-r}\right)''=\frac{2}{(1-r)^3}.
\end{split}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ejemplo}
Su esperanza es 
\begin{align*}
\begin{split}
E(X)=&\sum_{x=0}^{+\infty} x P(X=x)=\sum_{x=0}^{+\infty} x
\left(\frac{1}{2}\right)^{x+1}\\
= & 
\left(\frac{1}{2}\right)^2\sum_{x=1}^{+\infty} x
\left(\frac{1}{2}\right)^{x-1}=\left(\frac{1}{2}\right)^2
\frac{1}{\left(1-\frac{1}{2}\right)^2}=1.
\end{split}
\end{align*}



Ahora calculemos su función de distribución

\begin{align*}
\begin{split}
F_X(x)=& P(X\leq x)=\sum_{k=0}^x P(X=k)=\sum_{k=0}^x
\left(\frac{1}{2}\right)^{k+1}\\
=& \frac{\frac{1}{2}-\frac{1}{2}^{x+1}
\frac{1}{2}}{1-\frac{1}{2}}=1-(\frac{1}{2})^{x+1}.
\end{split}
\end{align*}
\end{frame}



\begin{frame}
\frametitle{Ejemplo}

Como la variable toma valores enteros positivos, podemos calcular su valor esperado
de esta otra manera

$$E(X)=\sum_{x=0}^{+\infty} (1-F_X(x))=\sum_{x=0}^{+\infty}(\frac{1}{2})^{x+1}=\frac{1}{2}
\frac{1}{1-\frac{1}{2}}=1.$$

Como ejercicio  calculad el valor esperado de la variable
$$Y=\mbox{número de intentos
para conseguir el color azul.}$$

\end{frame}

\subsection{Momentos de una variable aleatoria}
\begin{frame}
\frametitle{Momentos de una variable aleatoria}

\begin{definicion}
\begin{itemize}
\item Llamaremos  \emph{momento de orden $r$} respecto al punto $C$ a $E\left((X-C)^m\right)$.
\item Cuando $C=0$ los momentos reciben el nombre de \emph{momentos respecto al origen}.
\item Cuando $C=E(X)$ reciben el nombre de \emph{momentos centrales o respecto de la media}
\end{itemize}
\end{definicion}


Luego la esperanza es el momento de orden $1$ respecto al origen. 
Estos momentos son la versión poblacional de los momentos que vimos en el capítulo de estadística descriptiva, recibiendo estos último el nombre de momentos muestrales.

\end{frame}

\subsection{Varianza de una variable aleatoria}

\begin{frame}


\begin{itemize}
\item Hemos descrito el comportamiento aleatorio de una v.a. discreta mediante sus funciones  de probabilidad $P_{X}$ y de distribución $F_{X}$.
\item También tenemos un valor central; el valor esperado $E(X)$. 
\item Como medida básica nos queda definir una medida de lo lejos que están los datos del valor central $E(X)$ una de estas medidas es la varianza de $X$.
\end{itemize}
\end{frame}

\begin{frame}
\begin{definicion}
    Sea $X$ una v.a. Llamaremos \emph{varianza} de $X$ a

    $$Var(X)=E((X-E(X))^2)$$
\end{definicion}

Por lo tanto la varianza es el momento
      central de orden $2$.

    De forma frecuente se utiliza la notación $$\sigma_{X}^2=Var(X).$$
    
    A la raíz cuadrada positiva de la varianza
   $$\sigma_{X}=\sqrt{Var(X)}$$
   
   se la denomina desviación típica  o estándar de $X$.
\end{frame}

\begin{frame}

\frametitle{Propiedades de la varianza}
\begin{enumerate}[a)]
\item Si $X$ es una v.a. discreta con función de probabilidad $P_X$ su varianza es 
 $$\sigma_{X}^2=Var(X)=E((X-E(X))^2)=\sum_{x}(x-E(X))^2 P_{X}(x).$$
\item Sea $X$ una v.a.
 $$Var(X)=E(X^2)-(E(X))^2=\sum_{x} x^2 P_{X}(X)-(E(X))^2$$
\end{enumerate}
\end{frame}

\begin{frame}
\textbf{Demostración de b)}

\begin{align*}
%\begin{split}
Var(X)= & \sum_{x}(x-E(X))^2 P_{X}(x)\\
=& \sum_{x}(x^2 -2 x E(X)+(E(X)^2) P_{X}(x)\\
=& \sum_{x}x^2P_{X}(x) -  E(X)\sum_{x}2 x P_{X}(x)\\
 &+(E(X)^2)\sum_{x} P_{X}(x)\\
=& E(X^2)- 2 E(X) E(X) + (E(X))^2\\
=& E(X^2)-(E(X))^2.
%\end{split}
\end{align*}

\end{frame}

\begin{frame}

\frametitle{Ejemplo}
   Valculemos en  el ejemplo anterior la varianza del número de errores. Recordemos que:
   
    $$P(X=0)=0.42,\quad P(X=1)=0.4, \quad P(X=2)=0.18$$
    y que
    
    $$E(X)=0.76$$

    Entonces:
    
    $$Var(X)=E(X^2)-(E(X))^2 = E(X^2)-(0.76)^2$$
\end{frame}

\begin{frame}

\frametitle{Ejemplo}
  Ahora necesitamos calcular 
  
  $$E(X^2)= 0^2 (0.41)+ 1^2 (0.4)+ 2^2 (0.18)=0.4+0.72=1.12$$
  y por lo tanto
  
  $$Var(X)= E(X^2)-(0.76)^2=1.12-0.5776=0.542$$
  y $$\sqrt{Var(X)}=\sqrt{0.542}$$

  En resumen $\sigma_{X}^2=0.542$ y $\sigma_{X}=\sqrt{0.542}$
     
\end{frame}

\begin{frame}
\frametitle{Propiedades de la varianza}
\begin{enumerate}[a)]
\item $Var(X)\geq 0$
\item $Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0$
\item El mínimo de $E((X-C)^2)$ se alcanza cuando $C=E(X)$ y es $Var(X)$. Esta propiedad es una de las que hace útil a la varianza como medida de dispersión.
\end{enumerate}

\textit{Demostración:}(ejercicio)

\end{frame}


\subsubsection{Esperanza y varianza de transformaciones lineales}

\begin{frame}
\frametitle{Esperanza y varianza de transformaciones lineales. Propiedades}

\begin{definicion}
 Un \textbf{cambio de variable lineal} o \textbf{transformación lineal}
de una v.a. $X$ es otra v.a. $Y= a+ b X$  donde $a,b\in\RR$.
\end{definicion}

Sea $X$ una v.a. con
$E(X)=\mu_{X}$ y $Var(X)=\sigma_{X}^2$ y $a,b\in\RR$. Entonces si $Y=a+b X$ :
\begin{enumerate}[a)]
    \item $E(Y)=E(a + b X)=a+ b E(X)= a + b \mu_{X}$.
    \item $Var(Y)=Var(a+bX)=b^2 Var(X)= b^2 \sigma_{X}^2$
    \item $\sigma_{Y}=\sqrt{Var(Y)}=\sqrt{b^2 Var(X)}=|b| \sigma_{X}$
\end{enumerate}
\end{frame}

\begin{frame}
    
\textit{Demostración:}

\begin{align*}
\begin{split}
E(Y)=& E(a+bX)=\sum_{x}(a+b\cdot X)\cdot P_{X}(x)\\ =& 
a \sum_{x} P_{X}(x) + b \sum_{x} x\cdot P_{X}(x)\\ 
=& a + b\cdot E(X)=a + b \mu_{X}
\end{split}
\end{align*}

Las demostración de las demás propiedades queda como ejercicio.

\end{frame}


\end{document}
